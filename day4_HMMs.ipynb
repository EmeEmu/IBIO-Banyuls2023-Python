{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "203fb028-fc9d-4426-aae5-79b1b9d51868",
   "metadata": {},
   "source": [
    "Welcome to Day 4 of the Practicals !!\n",
    "\n",
    "Today we will investicate how to study sequences of behaviours. In perticular, how to use Markov Chains (MCs) and Hidden Markov Models (HMMs) to analyse the statistics of how zebrafish reorient themselves during free swim.\n",
    "\n",
    "The data we will be using comes from the paper : [Thermal modulation of Zebrafish exploratory statistics reveals constraints on individual behavioral variability, Le Goc et al. 2021, BMC Biol \\[1\\]](https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-021-01126-w). In this paper, the authors analyse the statiscs of how larvae re-orient themselves depending on the temperature of the water. As you might know, zebrafish larvae move in sequences of discrete bouts. With each bout, the larvae will move forward and potentially re-orient themselves. We want to understand this re-orientation statistics.\n",
    "\n",
    "![Setup](https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/img/free_swim_setup.png)\n",
    "\n",
    "![Convention](https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/img/free_swim_convention.png)\n",
    "\n",
    "We will not reproduce the same analysis as [1], but simplify the basic ideas.\n",
    "\n",
    "If you want to read more you can also look at [From behavior to circuit modeling of light-seeking navigation in zebrafish larvae, Karpenko et al. 2020, eLife \\[2\\]]( https://doi.org/10.7554/eLife.52882 ).\n",
    "\n",
    "\n",
    "During this tutorial, we will use the following markers :\n",
    "- ‚û°Ô∏èüë§‚å®Ô∏è : shows you what/were you need to code yourself.\n",
    "- üí° : shows hints to help you if you need it.\n",
    "- ‚ùó : shows important notes.\n",
    "- ‚ö†Ô∏è : shows warnings.\n",
    "\n",
    "‚ùó We remind you that at any time, you can run `?thing` to get the documentation for `thing`. Typicaly, this is useful if `thing` is a function, and you need to know what are its parameters and/or what it does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f73a2f-6f6f-4451-8b64-410103719493",
   "metadata": {},
   "source": [
    "# 0.0. Setup Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485ffd4-b4bd-4629-afd9-be18b57e8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /content/Helper_Functions/\n",
    "!wget -P /content/Helper_Functions/ https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/Helper_Functions/accessing_data.py\n",
    "!wget -P /content/Helper_Functions/ https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/Helper_Functions/hmm_plotters.py\n",
    "!wget -P /content/Helper_Functions/ https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/Helper_Functions/OrthoViewer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7502575-e8f4-4150-9ca3-5534a3fac5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d1630-fdf9-4c11-a7a6-349eebcc5e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipympl\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e3108-7a52-4cb7-9bfa-4716072aaeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --folder 1k21VhLoonOnoxxXyswrmE45VIB4FF00n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5c3b8-168a-4d15-8047-de2d220219a9",
   "metadata": {},
   "source": [
    "# 0.1. Imports\n",
    "\n",
    "As usual lets start by importing a few librairies and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aafa84-6e1b-485d-91c1-b82f711d2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.widgets import Slider\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "from hmmlearn import hmm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64571be5-5299-447b-a97a-0717256b168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper_Functions.accessing_data import h5tree_view\n",
    "from Helper_Functions.hmm_plotters import cmap_states, plot_transition_matrix, plot_angle_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2c044-4706-43e4-8180-91e6213eda86",
   "metadata": {},
   "source": [
    "# 0.2. Get the data\n",
    "\n",
    "And now lets load the data.\n",
    "\n",
    "You will find it in the hdf5 file `banyuls_data/behaviour_free_swimming.h5`. Let's load this file and show the data tree. You will see that it is split into 6 different temperature conditions (18¬∞C ‚Üí 33¬∞C).\n",
    "\n",
    "For each temperature, you have access to different datasets :\n",
    "- `bouttime` : the time of each bout.\n",
    "- `displacements` : the size of each bout.\n",
    "- `dtheta` : the change in orientation/direction of this bout.\n",
    "- `interboutintervals` : the time between bouts (i.e. how long the larva spent not moving).\n",
    "- `xpos` : the X position of the larvae\n",
    "- `ypos` : the Y position of the larvae\n",
    "\n",
    "Each of those datasets is 2D and structured `sequence x bout`. For example for 18¬∞C, 532 trajectories were recorded, containing 642 bouts. ‚ö†Ô∏è However, not all 532 trajectories were imaged long enough to observe 642 bouts! Sometimes the fish get out of the field of view. So the 6 datasets mentioned above contain NaNs !! Be careful during this practical, and remember to deal with the NaNs appropriatly !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be427c-9997-4c5c-a92d-c9dd63253087",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"banyuls_data/behaviour_free_swimming.h5\"\n",
    "file = h5py.File(filename, \"r\")\n",
    "h5tree_view(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa0049-afdb-42ce-af96-4934760732cf",
   "metadata": {},
   "source": [
    "# 0.3. Observe the Data\n",
    "\n",
    "Lets first focus on the data at 26¬∞C. Let's first load the data and plot an example fish trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2649c-a7d0-4ace-a7f5-abcfc8fc25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = file[\"/behaviour/26/\"]\n",
    "xs, ys = data[\"xpos\"][:], data[\"ypos\"]  # x and y positions \n",
    "dthetas = data[\"dtheta\"][:]             # reorientation angles\n",
    "dts = data[\"interboutintervals\"][:]     # interbout intervals\n",
    "distance = data[\"displacements\"][:]     # distance traveled with each bout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7293d5-e602-4cab-beee-eb502129b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.plot(xs[i], ys[i], color=\"grey\")\n",
    "h = ax.scatter(\n",
    "    xs[i], ys[i], c=np.insert(dthetas[i],-1,0), \n",
    "    s=np.insert(np.nan_to_num(dts[i]), 0, 0)*50+20,\n",
    "    cmap=\"coolwarm\", vmin=-100, vmax=+100)\n",
    "ax.scatter(xs[i,0], ys[i,0], marker=4, s=500, color=\"k\")\n",
    "\n",
    "fig.colorbar(h, ax=ax, label=\"dTheta (degrees)\", fraction=0.03)\n",
    "handles, labels = h.legend_elements(prop=\"sizes\", alpha=0.6, num=5, func=lambda x: (x-20)/50)\n",
    "legend2 = ax.legend(handles, labels, loc=\"upper right\", title=\"dT (s)\")\n",
    "\n",
    "ax.set_xlabel(\"X (mm)\")\n",
    "ax.set_ylabel(\"Y (mm)\")\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c3c3d-0420-47b6-8ced-ec0be9f9ed66",
   "metadata": {},
   "source": [
    "Look at a few trajactories by changing `i` in the previous cell. Do you understand how the larvae choose to reorient themselves ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd2fce-4cbc-462e-9574-aa060178f288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceae9e6b-3fd0-4571-b5b7-fa955dbb944d",
   "metadata": {},
   "source": [
    "# 1. Markov Chains\n",
    "\n",
    "In the first part of this practical, we will :\n",
    "1. manualy classify bouts to be one of 3 states : 'forward', 'left' or 'right'\n",
    "2. find the Markovian statistics of those 3 states\n",
    "3. see how those statistics evolve with temperature\n",
    "4. find the stationary distribution of the Markov Chain\n",
    "5. use the Markov Chain to generate new sequences of states "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aece19-4351-47e7-9c90-fd58b01e1f09",
   "metadata": {},
   "source": [
    "## 1.1 Classifying Bouts\n",
    "\n",
    "Let's start by looking at the re-orientation angles. Every time the larvae make a bout, they reorient their body by a certain angle $\\delta \\theta$.\n",
    "- if $\\delta \\theta \\approx 0$ : they will essentialy be going forward.\n",
    "- if $\\delta \\theta < 0$ : they will be going left.\n",
    "- if $\\delta \\theta > 0$ : they will be going right.\n",
    "\n",
    "Run the following cell to plot the histogram of $\\delta \\theta$. **‚û°Ô∏èüë§‚å®Ô∏èUsing the two sliders, can you find two thresholds which will define the 3 states : 'forward', 'left' and 'right' ?‚û°Ô∏èüë§‚å®Ô∏è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b207d74d-ff4b-49af-ae92-8285d3d8980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(dthetas.ravel(), bins=100, color=\"k\", density=True);\n",
    "left_line = ax.axvline(-100, color=cmap_states.colors[1])\n",
    "right_line = ax.axvline(+100, color=cmap_states.colors[2])\n",
    "ax.set_xlabel(r\"$\\delta \\theta$ (degree)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "\n",
    "fig.subplots_adjust(left=0.25, bottom=0.25)\n",
    "# Make a horizontal slider to control the frequency.\n",
    "axslidL = fig.add_axes([0.25, 0.1, 0.65, 0.03])\n",
    "left_slider = Slider(\n",
    "    ax=axslidL,\n",
    "    label='Left Threshold',\n",
    "    valmin=np.nanmin(dthetas),\n",
    "    valmax=0,\n",
    "    valinit=-100,\n",
    "    color=cmap_states.colors[1]\n",
    ")\n",
    "axslidR = fig.add_axes([0.25, 0.05, 0.65, 0.03])\n",
    "right_slider = Slider(\n",
    "    ax=axslidR,\n",
    "    label='Right Threshold',\n",
    "    valmax=np.nanmax(dthetas),\n",
    "    valmin=0,\n",
    "    valinit=+100,\n",
    "    color=cmap_states.colors[2]\n",
    ")\n",
    "\n",
    "def update(val):\n",
    "    left_line.set_xdata([left_slider.val])\n",
    "    right_line.set_xdata([right_slider.val])\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "left_slider.on_changed(update)\n",
    "right_slider.on_changed(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10b36e-c0c4-4a31-b178-0b6e856a4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "left, right = ..., ... # the two thresholds you found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa90de3-ecce-4394-8ec9-d3d5d61dbb69",
   "metadata": {},
   "source": [
    "‚û°Ô∏èüë§‚å®Ô∏èNow lets **write a function to classify bouts according to the thresholds.** ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "\n",
    "It should take $\\delta \\theta$s as argument and return :\n",
    "- 0 for 'forward'\n",
    "- 1 for 'left'\n",
    "- 2 for 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b04a8d-296e-4160-a7f4-73d2ac70e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bout_classifier(dtheta_seqs, left_threshold, right_threshold):\n",
    "    \"\"\"Classify re-orientation angles into 3 states based on 2 thresholds.\n",
    "\n",
    "    Re-orientation angle Œ¥Œ∏ are classifed as followed :\n",
    "        - Œ¥Œ∏ < :left_threshold:  ‚Üí 1\n",
    "        - Œ¥Œ∏ > :right_threshold: ‚Üí 2\n",
    "        - :left_threshold: < Œ¥Œ∏ < :right_threshold: ‚Üí 0\n",
    "        - Œ¥Œ∏ = NaN ‚Üí -1\n",
    "    ‚ö†Ô∏è Be carefull of the NaNs !! \n",
    "    \n",
    "    Parameters :\n",
    "    ------------\n",
    "    dtheta_seqs : array\n",
    "        sequences of re-orientation angles\n",
    "    left_threshold : number\n",
    "        lower threshold\n",
    "    right_threshold : number\n",
    "        upper threshold\n",
    "\n",
    "    Returns :\n",
    "    ---------\n",
    "    bout_seqs : array of integers\n",
    "        sequence of states.\n",
    "    \"\"\"\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "    ...\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "\n",
    "    assert bout_seqs.shape == dtheta_seqs.shape, \":bout_seqs: should have the same shape as :dtheta_seqs:.\"\n",
    "    assert \"int\" in bout_seqs.dtype.name, \":bout_seqs: should be an array of integers.\"\n",
    "    return bout_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac5149-3e3e-4af3-b047-a8ef64d7842a",
   "metadata": {},
   "source": [
    "‚û°Ô∏èüë§‚å®Ô∏è Let's **apply your classification function to the data** ‚û°Ô∏èüë§‚å®Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d3cc1-c45e-4fe6-886d-ebfe40e7ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "bouts = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223de51-1179-480b-8cd7-f1130b1650e4",
   "metadata": {},
   "source": [
    "Now, let's plot the histograms of re-orientation angles for each state to check that our classification worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951c946-502b-4432-a8d0-906ca652c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for s in range(3):\n",
    "    ax.hist(dthetas[bouts==s], bins=100, range=[-180, +180],density=True, color=cmap_states.colors[s]);\n",
    "ax.set_xlabel(r\"$\\delta \\theta$ (degree)\")\n",
    "ax.set_ylabel(\"Density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bab5be-68bf-41d5-a63f-958b943dc4e3",
   "metadata": {},
   "source": [
    "And let's see what it looks like for an example sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a97e3b-accd-430c-92c5-db1c554d017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "plot_angle_sequence(ax, dthetas[i], states=bouts[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66ef5b3d-d39b-4f12-86f8-a21508ef8ed8",
   "metadata": {},
   "source": [
    "## 1.2 Bout Probabilities and transitions\n",
    "\n",
    "Now that we have classifed the bouts into 3 possible states, we can define a Markov Chain to study the statistics of those states and the transitions between those states.\n",
    "\n",
    "**A quick reminder on Markov Chains (MCs)** \n",
    "\n",
    "The idea of a MC is that the next state is only determined from the current state. This is done through the Transition Matrix which gives you the probabilities of all possible next states, given the current state. So in effect, it is a system which has no memory.  \n",
    "In this tutorial, we will write the probability of the next state given the current state as : $P(\\text{next}|\\text{current}) = P(\\text{current}‚Üí\\text{next}) = P(c‚Üín)$\n",
    "\n",
    "![Markov](https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/img/Markov.png)\n",
    "\n",
    "‚û°Ô∏èüë§‚å®Ô∏è We will start by first **creating a function which computes the *a priori* probability of each state from the data** ‚û°Ô∏èüë§‚å®Ô∏è : $$P(s)\\ ,\\ s\\in{F,L,R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49ca3d-5849-4d37-9d37-a6ff5e986115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bout_proba(bout_seqs):\n",
    "    \"\"\"Compute the a priori probability of each state.\n",
    "\n",
    "    üí° you can use the function np.unique(X, return_counts=True) to count \n",
    "    the occurence of every unique item in an array.\n",
    "\n",
    "    ‚ö†Ô∏è the input might contain values -1 (remember the previous exercise). \n",
    "    Your function should work with and without those -1 values !\n",
    "    \n",
    "    Parameters :\n",
    "    ------------\n",
    "    bout_seqs : array of integers\n",
    "        sequence of states. 0='Forward', 1='Left', 2='Right', -1=NaN\n",
    "\n",
    "    Returns :\n",
    "    ---------\n",
    "    p_bout : array of 3 float\n",
    "        probability of each state 'Forward', 'Left', 'Right'\n",
    "    \"\"\"\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "    ...\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "    \n",
    "    assert p_bout.ndim == 1, \":p_bout: should be a 1d array with 3 floats.\"\n",
    "    assert len(p_bout)==3, \":p_bout: should be a 1d array with 3 floats.\"\n",
    "    assert p_bout.sum() == 1, \":p_bout: is a probability array. It should be normalised to 1.\"\n",
    "    return p_bout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a59789-7aee-40fd-8739-f541b35b379e",
   "metadata": {},
   "source": [
    "‚û°Ô∏èüë§‚å®Ô∏è **Let's apply this function to our sequence of bouts to see the $P(s)$** ‚û°Ô∏èüë§‚å®Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd18ad7-4ff6-47f6-a387-4994e96d0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "p_bouts = ...\n",
    "p_bouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739516f3-a356-4c43-8405-199b6fe09ee8",
   "metadata": {},
   "source": [
    "As $P(s)$ is the *a priori* probability of being in each state, $\\sum_{s\\in{F,L,R}}P(s)=1$. **‚û°Ô∏èüë§‚å®Ô∏è Check that this is true of `p_bouts` ‚û°Ô∏èüë§‚å®Ô∏è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6ec6e-43ba-425c-bcb5-227738f42a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac725545-633d-42e2-8438-15cca1d9e82f",
   "metadata": {},
   "source": [
    "‚û°Ô∏èüë§‚å®Ô∏è Now we will **create a function to compute the Transition Probability Matrix.** ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "\n",
    "This is a bit more complicated than $P(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab71bd-b303-426d-9e2f-3f61066a4336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bout_transition(bout_seqs):\n",
    "    \"\"\"Compute the transition probability matrix from sequences of bouts.\n",
    "\n",
    "    üí° you need to look at each sequence one by one. For each bout, identify\n",
    "    the next bout, this gives you the transition, and record the total \n",
    "    number of times you have seen each transition. You will then need to\n",
    "    normalise the matrix correctly.\n",
    "\n",
    "    ‚ö†Ô∏è the input might contain values -1 (remember the previous exercise). \n",
    "    Your function should work with and without those -1 values !\n",
    "    \n",
    "    Parameters :\n",
    "    ------------\n",
    "    bout_seqs : 2D array of integers, shape (sequence x bouts)\n",
    "        sequences of states. 0='Forward', 1='Left', 2='Right', -1=NaN\n",
    "\n",
    "    Returns :\n",
    "    ---------\n",
    "    P : 2D array of shape (3,3)\n",
    "        probability of transition between each state 'Forward', 'Left' and 'Right'.\n",
    "    \"\"\"\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "    ...\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "    \n",
    "    assert P.shape == (3,3), \":P: should be a 2D matrix of shape 3x3.\"\n",
    "    assert np.isclose(P.sum(axis=1), 1).all(), \":P: is not normalised properly.\"\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21be87c6-4db9-422b-8055-c96e7bd42a29",
   "metadata": {},
   "source": [
    "**‚û°Ô∏èüë§‚å®Ô∏è Let's apply this function to our sequence of bouts ‚û°Ô∏èüë§‚å®Ô∏è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd813a54-6fab-4ace-97f3-4a49d4540505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "T_bouts = ...\n",
    "T_bouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b899465c-5e53-4b37-a2be-d0fdccbdad87",
   "metadata": {},
   "source": [
    "As the transition matrix gives you the probability of going to the next state $n$ from the current state $c$ : $P(c‚Üín)$, it should have the following normalisation : $$\\sum_{n\\in{F,L,R}}P(c‚Üín)=1\\ \\ \\forall c$$\n",
    "**‚û°Ô∏èüë§‚å®Ô∏è Check that this is the case for `T_bouts` ‚û°Ô∏èüë§‚å®Ô∏è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb069a66-1189-43e7-8acf-b3bea44233f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3a7c5-da2d-4d79-a8f8-fa08d09feabc",
   "metadata": {},
   "source": [
    "Now that was had work ! üòÆ‚Äçüí®üòÆ‚Äçüí®  Well Done !! üòÉ \n",
    "\n",
    "As a reward, here is a plot of what we have found so far : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63621897-7d46-4274-ab43-00b0735ceaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(15,5))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.set_title(r\"P(s)\")\n",
    "ax.bar([0,1,2], p_bouts, color=cmap_states.colors)\n",
    "ax.set_xticks([0,1,2], [\"forward\", \"left\", \"right\"])\n",
    "ax.set_ylabel(\"Bout Probability\")\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "ax.set_title(r\"P(c‚Üín)\")\n",
    "h = plot_transition_matrix(ax, T_bouts)\n",
    "fig.colorbar(h, ax=ax, label=\"Transition Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda9ae7-ade1-4050-ab49-14254d31981d",
   "metadata": {},
   "source": [
    "Spend a bit of time interpreting those 2 graphs. What do they tell you about the re-orientation dynamics of the larvae ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d3bdb-5d82-44b9-9c86-e506386686f0",
   "metadata": {},
   "source": [
    "## 1.3 Comparing Different temperatures\n",
    "\n",
    "Now that we have seen how to :\n",
    "- classify the bouts\n",
    "- compute $P(s)$\n",
    "- compute $P(c‚Üín)$\n",
    "\n",
    "for the dataset at 26¬∞C, we will do the same for the other temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e9534-2157-494c-849c-4e51dfca1df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperatures = [18,22,26,30,33]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357bf695-07e4-47c8-87d8-4f8369b7c1e7",
   "metadata": {},
   "source": [
    "**‚û°Ô∏èüë§‚å®Ô∏è Use the 3 function you wrote earlier to compute $P(s)$ and $P(c‚Üín)$ for each temperature. ‚û°Ô∏èüë§‚å®Ô∏è**\n",
    "\n",
    "Store the results in 2 array :\n",
    "- `p_alltemp` of shape (temperatures x 3)\n",
    "- `T_alltemp` of shape (temperatures x 3 x 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a619190f-a6f5-4cc3-9d7b-4828cf15550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "p_alltemp = np.empty((len(Temperatures), 3))\n",
    "T_alltemp = np.empty((len(Temperatures), 3, 3))\n",
    "for t,T in enumerate(Temperatures):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc92ba54-1469-4575-8d53-0819f4999e05",
   "metadata": {},
   "source": [
    "Now we will plot how $P(s)$ varies with temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a8281-f890-413b-9c42-97cb14262115",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(Temperatures, p_alltemp[:,0], \"-o\", color=cmap_states.colors[0], label=\"forward\")\n",
    "ax.plot(Temperatures, p_alltemp[:,1], \"-o\", color=cmap_states.colors[1], label=\"left\")\n",
    "ax.plot(Temperatures, p_alltemp[:,2], \"-o\", color=cmap_states.colors[2], label=\"right\")\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_xticks(Temperatures)\n",
    "ax.set_xlabel(\"Temperature (¬∞C)\")\n",
    "ax.set_ylabel(\"P(bout)\")\n",
    "ax.set_title(r\"P(s)\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3d819-bcd2-4495-a546-b3a8e6b75f19",
   "metadata": {},
   "source": [
    "And now how $P(c‚Üín)$ varies with temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e76ea-5fbb-47b6-85a6-e08f65d698c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axin = ax.inset_axes([i-0.4, j-0.4, 0.8, 0.8], transform=ax.transData)\n",
    "        if (i,j) != (2,2):\n",
    "            axin.set_xticks([])\n",
    "            axin.set_yticks([])\n",
    "        else:\n",
    "            axin.set_xlabel(\"Temperature (¬∞C)\")\n",
    "            axin.set_ylabel(\"P\")\n",
    "        axin.spines['right'].set_visible(False)\n",
    "        axin.spines['top'].set_visible(False)\n",
    "        axin.set_ylim(0,1)\n",
    "        axin.plot(Temperatures, T_alltemp[:,i,j], \"-o\",color=\"k\")\n",
    "        \n",
    "ax.set_xticks([0,1,2], [\"forward\", \"left\", \"right\"])\n",
    "ax.set_yticks([0,1,2], [\"forward\", \"left\", \"right\"])\n",
    "ax.set_xlabel(\"Current step\")\n",
    "ax.set_ylabel(\"Next step\")\n",
    "ax.set_xlim(-0.5,2.5)\n",
    "ax.set_ylim(-0.5,2.5)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(r\"P(c‚Üín)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1cfd93-022f-4064-8869-a20781039ce9",
   "metadata": {},
   "source": [
    "How do you interpret those results ? What does this mean for the larvae behaviour ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cf012-549a-4bf4-b4a2-146d1e7f7b06",
   "metadata": {},
   "source": [
    "## 1.4 Stationary Distriution\n",
    "\n",
    "An important property of a Markov Chain is its stationary distribution $\\pi_s$. In this section we will compute it using 2 different methods, and compare it to the $P(s)$ we computed from the data.\n",
    "\n",
    "**First lets have a quick reminder about Stationary Distributions (SDs) :**\n",
    "\n",
    "The SD of a MC can be seen as the steady state regime of that MC. It is the probability of states $\\pi = [P(s_1), ..., P(s_n)] = [\\pi_{s_1}, ...\\pi_{s_n}]$ which will not change given the transition matrix $T_{cn}=P(c‚Üín)$. Mathematicaly this is expressed as $$\\pi T = \\pi$$\n",
    "\n",
    "**Why is it useful to know $\\pi_s$ ?**  \n",
    "Well it tells you that, whatever your initial state, over the long run, the Markov Chain will be in state $s$ with a probability $\\pi_s$.\n",
    "\n",
    "**How do you find $\\pi_s$ ?**  \n",
    "There are 2 methods :\n",
    "1. solve the equation $\\pi T = \\pi$ to find $\\pi$. This will give you a quasi-exact solution, but can be quite math-intensive\n",
    "2. by mutiplying $T$ with itself $n$ times, you are computing the transition probability from the current step to n steps in the future $T^n = T\\times T\\times...\\times T$. It can be shown that when $n‚Üí\\infty$, $T^n_c = \\pi\\ \\ \\forall c$.\n",
    "\n",
    "Don't worry if this is not clear yet, we will use both methods down bellow and you will understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78cf212-25b0-40de-88ef-4b1466930f5c",
   "metadata": {},
   "source": [
    "Lets start with method 1. solving $\\pi T = \\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851f366-fb41-4c84-a392-623ebb6c117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvals, eigenvects = np.linalg.eig(T_bouts.T)      # find eigen of matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcfa61c-0c4c-49cd-b713-6aca3d8a01f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_to_1_idx = np.isclose(eigenvals,1)              # find the eigen value which is close to 1\n",
    "target_eigenvect = eigenvects[:,close_to_1_idx][:,0]  # find the respective eigen vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e736018-0c8a-4507-b73b-b5b99f42e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_1 = target_eigenvect / sum(target_eigenvect)       # normalise the eigen vector vector to get a probability\n",
    "pi_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df23bd95-70b2-49a6-98ee-43663c32e30b",
   "metadata": {},
   "source": [
    "‚û°Ô∏èüë§‚å®Ô∏è now **check that `pi_1` is indeed a stationary distribution of `T_bouts` ‚û°Ô∏èüë§‚å®Ô∏è**\n",
    "\n",
    "üí° To check that $\\pi T = \\pi$, you can check that $\\pi T - \\pi \\approx \\vec{0}$ .  \n",
    "üí° $\\pi T$ requires a matrix multiplication which can be made with the numpy `@` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee7d93-b04a-4647-840e-c97e4d770336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6f9e6-e49e-4f11-a232-5ba1702cbcfe",
   "metadata": {},
   "source": [
    "Now let's try method 2 :  \n",
    "**‚û°Ô∏èüë§‚å®Ô∏è Compute $T^n$ ‚û°Ô∏èüë§‚å®Ô∏è**\n",
    "\n",
    "üí° you can use the function `np.linalg.matrix_power` to raise a array to a power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28f4e0-53f2-4bf6-90e9-0f981d31088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "T_power = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a4a7f-3f57-43a8-abcf-784c06172bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "h = plot_transition_matrix(ax, T_power)\n",
    "fig.colorbar(h, ax=ax, label=\"Transition Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546215c4-f67b-486c-9ee4-9be7e376445e",
   "metadata": {},
   "source": [
    "As you can see, if you choose $n$ big enoug all the columns of `T_power` are the same.  \n",
    "**‚û°Ô∏èüë§‚å®Ô∏è Isolate one column as `pi_2` and check that it is indeed a stationary distribution of `T_bouts` ‚û°Ô∏èüë§‚å®Ô∏è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc2487-5df3-4413-bf53-0e69abe1b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "pi_2 = ...\n",
    "pi_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06004c75-9be3-4a7a-a2c4-f0a1cf635b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426a835-cf84-4459-9280-aefe2d69b597",
   "metadata": {},
   "source": [
    "‚û°Ô∏èüë§‚å®Ô∏è Now **compare `pi_1`, `pi_2` and `p_bouts`. What can you conclude ? ‚û°Ô∏èüë§‚å®Ô∏è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17b71f-c2c4-4bf2-aa00-040b549ec513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61051a16-2530-438c-9705-848958aa0142",
   "metadata": {},
   "source": [
    "## 1.5 Generate a sequence of bouts\n",
    "\n",
    "An important aspect of Markov Chains is that they are generative models. Indeed, once you know the transition matrix $T_{cn} = P(c‚Üín)\\ \\ c,n\\in \\{F,L,R\\}$, you can perform the follwing algorithm to generate a sequence of states :\n",
    "1. choose an initial state $s_1$\n",
    "2. from $T_{cn}$ find $P(s_1‚Üín)$ the probabilities of transitioning to each state from $s_1$\n",
    "3. find the next state $s_2 \\sim P(s_1‚Üín)$ by sampling $P(s_1‚Üín)$\n",
    "4. repeat 2. and 3. to find $P(s_2‚Üín)$ and $s_3 \\sim P(s_2‚Üín)$\n",
    "5. ... do that N-1 times\n",
    "6. you have generated a sequence of states $[s_1, s_2, ..., s_N]$\n",
    "\n",
    "‚ùó**Notes :**  \n",
    "- $T_{cn}$ is a matrix (it is 2D), but $P(s_1‚Üín)$ is a vector (it is 1D). It's the column $s_1$ of $T$.  \n",
    "- $P(s_1‚Üín)$ is a probability vector, so it is normalised $\\sum_{n\\in \\{F,L,R\\}}P(s_1‚Üín) = 1$\n",
    "- We can sample a state from any probability vector. For example, to choose the initial state $s_1$, it is common practice to sample from the *a priori* state probability $P(s)$ with $s_1 \\sim P(s)$.\n",
    "\n",
    "In our case, we know the transition matrix `T_bouts` between the states 'Forward', 'Left' and 'Right', and we will now use it to generate fake behavioural sequences of re-orientations.\n",
    "\n",
    "‚û°Ô∏èüë§‚å®Ô∏è To start, we will first **write a function which performs the sampling from any probability vector ‚û°Ô∏èüë§‚å®Ô∏è**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32745b83-5f06-4489-8e9a-2fc7e6d1c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_state_from_proba(p, n=None):\n",
    "    \"\"\"Sample a probability vector.\n",
    "\n",
    "    üí° You can use the function np.random.choice to perform the sampling.\n",
    "\n",
    "    Parameters :\n",
    "    ------------\n",
    "    p : 1d array\n",
    "        probability vector\n",
    "    n : interger or None\n",
    "        number of samples to make. If None, will sample only once.\n",
    "        default : None\n",
    "\n",
    "    Return :\n",
    "    --------\n",
    "    s : 1d array or interger\n",
    "        sampled state(s)\n",
    "    \"\"\"\n",
    "    assert p.ndim==1, \":p: should be a 1D vector.\"\n",
    "    assert p.sum()==1, \":p: should be normalised to 1.\"\n",
    "    \n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "    ...\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "\n",
    "    if n is None:\n",
    "        assert \"int\" in str(type(s)), \":n:=None therefore :s: should be an integer.\"\n",
    "    else:\n",
    "        assert s.ndim==1, \":s: should be a 1D vector.\"\n",
    "        assert len(s)==n, \":s: should be of lenght :n:.\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4f2c1-c43a-4416-9c6f-67a47a847a8f",
   "metadata": {},
   "source": [
    "Now let's check that, if we sample 1 million times from the *a priori* probability $P(s)$ we computed earlier `p_bouts`, we get the same fraction of each state 'Forward','Left', and 'Right'.\n",
    "\n",
    "**‚û°Ô∏èüë§‚å®Ô∏è First sample (with `n` = 1 million) the probability vector `p_bouts`. ‚û°Ô∏èüë§‚å®Ô∏è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48d52f-5294-4e8d-a36a-0eed243696c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "samples = ...\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372dff3-9432-407f-aad6-12a667eaf0f9",
   "metadata": {},
   "source": [
    "Now we use the `bout_proba` function we defined earlier to compute the *a priori* probability of each state in the sampled data. And we compare with `p_bouts`.  \n",
    "If your function `rand_state_from_proba` function is correct, the original and sampled data should have the same *a priori* state probabilities. **Check that this is the case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003505e-b871-4d69-b5dd-5f159ebc8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sampled = bout_proba(samples)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "width = 0.3\n",
    "x = np.arange(3)\n",
    "ax.bar(x-width/2, p_bouts, width, label=\"Data\", color=\"k\")\n",
    "ax.bar(x+width/2, p_sampled, width, label=\"Sampled\", color=\"orange\")\n",
    "ax.legend()\n",
    "ax.set_xticks([0,1,2], [\"forward\", \"left\", \"right\"])\n",
    "ax.set_ylabel(\"Bout Probability\")\n",
    "ax.set_title(r\"P(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ade87-10e6-4502-a710-599495165d60",
   "metadata": {},
   "source": [
    "‚û°Ô∏èüë§‚å®Ô∏èNow that we have a way to sample from a probability vector, **write a function which implements the generation algorithm descibed above. ‚û°Ô∏èüë§‚å®Ô∏è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85047e8-dd7e-45ad-8c5f-56efb2e5edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(N, p, T):\n",
    "    \"\"\"Generate a sequence from a Markov Chain.\n",
    "\n",
    "    Parameters :\n",
    "    ------------\n",
    "    N : interger\n",
    "        number of steps to generate.\n",
    "    p : 1d array\n",
    "        a priori probability vector of states.\n",
    "    T : 2d array\n",
    "        transition probability matrix between states.\n",
    "\n",
    "    Return :\n",
    "    --------\n",
    "    seq : 1d array of ints, lenght :N:\n",
    "        sequence of generated states\n",
    "    \"\"\"\n",
    "    assert p.ndim==1, \":p: should be a 1d vector.\"\n",
    "    assert T.ndim==2, \":T: should be a 2D matrix.\"\n",
    "    assert len(p) == T.shape[0] == T.shape[1], \":p: and :T: have incompatible shapes.\"\n",
    "    assert p.sum()==1, \":p: should be a normalised probability vector.\"\n",
    "    assert (T.sum(axis=1)==1).all(), \":T: should be a properly normalised transition matrix.\"\n",
    "\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "    ...\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "\n",
    "    assert seq.ndim==1, \":seq: should be a 1d vector.\"\n",
    "    assert len(seq)==N, \":seq: should have a lenght of :N:.\"\n",
    "    assert \"int\" in seq.dtype.name, \":seq: should be a vector of integers.\"\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b964f-056f-4f44-831b-bb262611313b",
   "metadata": {},
   "source": [
    "**‚û°Ô∏èüë§‚å®Ô∏è Now use this new function to generate 10 thousand steps from `T_bouts` and `p_bouts` ‚û°Ô∏èüë§‚å®Ô∏è**  \n",
    "Then run the new cell to check that the statistics of the generated sequence is the same as in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d155655-7bea-4402-bfca-aa1bae5e69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "sequence = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cae714-313d-47b0-94d3-c9427ba7c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(15,5))\n",
    "\n",
    "ax = axs[0]\n",
    "width = 0.3\n",
    "x = np.arange(3)\n",
    "ax.bar(x-width/2, p_bouts, width, label=\"Data\", color=\"k\")\n",
    "ax.bar(x+width/2, bout_proba(sequence), width, label=\"Generated\", color=\"orange\")\n",
    "ax.legend()\n",
    "ax.set_xticks([0,1,2], [\"forward\", \"left\", \"right\"])\n",
    "ax.set_ylabel(\"Bout Probability\")\n",
    "ax.set_title(r\"P(s)\")\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot([0,1], [0,1], color=\"grey\", zorder=0)\n",
    "ax.scatter(T_bouts, bout_transition([sequence]), color=\"k\")\n",
    "ax.set_xlabel(\"Data\")\n",
    "ax.set_ylabel(\"Generated\")\n",
    "ax.set_title(r\"P(c‚Üín)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f566b0-5e9f-4943-b17c-4231384a1e4a",
   "metadata": {},
   "source": [
    "‚û°Ô∏èüë§‚å®Ô∏è Now that we know our Markov Chain generation works, **generate a smaller number of steps and plot them to see what it looks like.** ‚û°Ô∏èüë§‚å®Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757bd99-ef67-4482-9541-00463ccf6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "sequence = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9f634-e8d3-40a8-a2fb-37a0a0b63d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "angle = sequence.astype(float).copy()\n",
    "angle[sequence==2] = right\n",
    "angle[sequence==1] = left\n",
    "plot_angle_sequence(ax, angle, states=sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25db3418-17b6-46df-b100-05f5f0074f07",
   "metadata": {},
   "source": [
    "# 2. (optional) Gaussian Mixture\n",
    "\n",
    "‚ùó This section is optional. If you have less than 1h left before the end of the practical, move to the [next section](#hidden-markov-model).\n",
    "\n",
    "Previously, we classified bouts based on manual thresholds. This might be good enough in certain cases such as this one, but it has the disadvantage of being very arbitrary.  \n",
    "\n",
    "In this section we will see a method called **Gaussian Mixture Model**.  \n",
    "The idea of a Mixture Model is to identify subpopulations while only looking at the overall population. Typicaly in Gaussian Mixture Model, we make the assumption that every subpopulation follows a normal distribution $\\mathcal{N(\\mu,\\sigma)}$.  \n",
    "What is good with Mixture Models, is that they are unsupervised, therefore the choice of which observation belongs in which subpopulation is no longer arbitrary.\n",
    "\n",
    "![GaussianMixtureModel](https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/img/Mixture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86438b1f-19fa-424b-b429-aa2f5686e366",
   "metadata": {},
   "source": [
    "First let's reload the data for 26¬∞C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b66031-0bd6-4f42-ba6d-dccba92f03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = file[\"/behaviour/26/\"]\n",
    "xs, ys = data[\"xpos\"][:], data[\"ypos\"]  # x and y positions \n",
    "dthetas = data[\"dtheta\"][:]             # reorientation angles\n",
    "dts = data[\"interboutintervals\"][:]     # interbout intervals\n",
    "distance = data[\"displacements\"][:]     # distance traveled with each bout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60dcd33-9835-4cc9-af73-1501148d2a0b",
   "metadata": {},
   "source": [
    "Let's look again at the distribution of re-orientation angles, and try to use Gaussian Mixture Modeling to find our 3 states 'Forward', 'Left', and 'Right'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9873ffbe-ac75-424c-803c-7eb5cc2a5b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(dthetas.ravel(), bins=100, color=\"k\", density=True);\n",
    "ax.set_xlabel(r\"$\\delta \\theta$ (degree)\")\n",
    "ax.set_ylabel(\"Density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb524ea-a923-49d1-80ee-381e8713167f",
   "metadata": {},
   "source": [
    "It is clear here that there are 3 subpopulations, corresponding to 3 gaussian-like distributions with means $\\mu \\approx [0,-40,+40]$.  \n",
    "We will therefore create a Gaussian Mixture Model with 3 componants (aka. subpopulations), and provide it with an initial guess of the means. We do this for 2 reasons :\n",
    "1. so that the learning algorithm converges faster\n",
    "2. to force the subpopulation #0 to be the 'Forward', subpopulation #1 to be the 'Left', etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9c5f9-3eee-432f-9858-22108268c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=3, means_init=np.array([[0],[-40],[+40]]))\n",
    "gm.fit(dthetas[np.isfinite(dthetas)][:,np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d005f-55ec-4dfb-b346-e6bbb4fed52b",
   "metadata": {},
   "source": [
    "Let's look at what parameters were found by the model.  \n",
    "first the means of the subpopulations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e3648-3972-45a1-86d0-865f6c25e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.means_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940799a1-3f90-433c-8e7a-73d8ce65058b",
   "metadata": {},
   "source": [
    "then the variances ($=\\sigma^2$) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a28f1-9444-46a7-acf1-cd8841487d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.covariances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd71fe-4ed4-4e5d-b214-9fdd3a72b84b",
   "metadata": {},
   "source": [
    "and how much each subpopulation participates in the overall population (aka. the weights) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed1327-0507-4138-9d8b-33fdad001df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.weights_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b819b9-4c4b-4788-a2a8-d07aa4eb067c",
   "metadata": {},
   "source": [
    "Let's look at what those subpolulation's distributions look like :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa3f9d-8270-4ae6-b1b5-6ce626ffb2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(np.nanmin(dthetas), np.nanmax(dthetas), 1000)\n",
    "ax.hist(dthetas.ravel(), bins=100, color=\"grey\", density=True, stacked=False,label=\"Data\");\n",
    "ax.plot(\n",
    "    x, \n",
    "    norm(loc=gm.means_[0], scale=np.sqrt(gm.covariances_[0,0])).pdf(x) * gm.weights_[0], \n",
    "    color=cmap_states.colors[0], \n",
    "    label=\"Subpopulation 'Forward'\",\n",
    "    linewidth=3,\n",
    ")\n",
    "ax.plot(\n",
    "    x, \n",
    "    norm(loc=gm.means_[1], scale=np.sqrt(gm.covariances_[1,0])).pdf(x) * gm.weights_[1], \n",
    "    color=cmap_states.colors[1],\n",
    "    label=\"Subpopulation 'Left'\",\n",
    "    linewidth=3,\n",
    ")\n",
    "ax.plot(\n",
    "    x, \n",
    "    norm(loc=gm.means_[2], scale=np.sqrt(gm.covariances_[2,0])).pdf(x) * gm.weights_[2], \n",
    "    color=cmap_states.colors[2],\n",
    "    label=\"Subpopulation 'Right'\",\n",
    "    linewidth=3,\n",
    ")\n",
    "ax.set_xlabel(r\"$\\delta \\theta$ (degree)\")\n",
    "ax.set_ylabel(\"PDF\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e82f91-aff6-43ae-a7e8-2484a9963018",
   "metadata": {},
   "source": [
    "With Mixture Models, once they are trained, you can ask them to classify in which subpopulation is each observation.  \n",
    "Let's first do it on all observations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c4bec-e2df-4b34-b8fb-766f916642c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, bins = np.histogram(dthetas[np.isfinite(dthetas)], bins=500)\n",
    "h = h/h.sum()\n",
    "predicted = gm.predict(bins[:,np.newaxis])\n",
    "colors = [{p<0.5:cmap_states.colors[0], 0.5<=p<=1.5: cmap_states.colors[1], p>1.5: cmap_states.colors[2]}[True] for p in predicted]\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(bins[:-1], h, width=np.diff(bins), align=\"edge\", color=colors)\n",
    "ax.set_xlabel(r\"$\\delta \\theta$ (degree)\")\n",
    "ax.set_ylabel(\"PDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d08981-d2a8-4fe5-92a3-65b8831ce9ad",
   "metadata": {},
   "source": [
    "‚û°Ô∏èüë§‚å®Ô∏è **Now extract from `dthetas` a single re-orientation sequence (remember to remove the NaNs), and use the Gaussian Mixture Model to predict in which subpopulation is each observation.** ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "\n",
    "üí° you can use the function `gm.predict`.  \n",
    "üí° `gm.predict` expects a 2D array, you can use `np.newaxis` to solve this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa078d-3174-440a-8170-bdb026f903fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "sequence = ...\n",
    "...\n",
    "predicted_states = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207001f3-7046-4822-b7a1-a60a2a62b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "plot_angle_sequence(ax, sequence, states=predicted_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb24e1-d7b3-4c4f-a1d6-8dd45eecd69a",
   "metadata": {},
   "source": [
    "Because Mixture Models are probabilistic, they can also ask for the probability for an observation to be in each subpopulation.\n",
    "\n",
    "‚û°Ô∏èüë§‚å®Ô∏è **Using the sequence as above, compute the probability that each observation is in the 'Left' subpopulation** ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "\n",
    "üí° you can use the `gm.predict_proba` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b999aa5-9370-47ac-9199-f7989f7bdaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "predicted_proba = ...\n",
    "proba_left = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558a876-d4cc-44ac-af13-c2138de28cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "plot_angle_sequence(ax, sequence, probas=proba_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a62795-37b7-49ba-b890-731acc7eb4ae",
   "metadata": {},
   "source": [
    "According to you, how well does it work ?  \n",
    "Try different sequences and look at the 'Forward' and 'Right' subpopulations.  \n",
    "Is there a problem around the 'Forward' subpopulation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08fe189-6492-4142-a0ea-14452d65dc15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fab6025-0575-4132-a69f-a6a3febe6b5d",
   "metadata": {},
   "source": [
    "# 3. Hidden Markov Model\n",
    "\n",
    "In section 1 we've seen how we can model the statistics of re-orientations angles $\\delta \\theta$s using a Markov Chain (MC). However, for this we had to first classify manualy the bouts into discrete states 'Forward', 'Left' and 'Right'.  \n",
    "In section 2 we've seen that we can more or less model the re-orientations $\\delta \\theta$s by the mixture of 3 gaussians, one for each state.\n",
    "\n",
    "In this last section, we will use Hidden Markov Models (HMMs) to once again model the re-orientation statistics of the larvae.\n",
    "\n",
    "**A quick reminder about HMMs :**  \n",
    "(‚ùó We will only talk about very simple 1D HMMs in this reminder.)  \n",
    "A HMM works very similarly to a MC. It makes the assumption that the system can be in one of several discrete states, and has a transition matrix $T_{cn} = P(c‚Üín)$ which defines how the system transitions from the current state to the next state.  \n",
    "However, contrary to the MC which only deals with the discrete states, with HMMs, we concider that the states are hidden (hence the name üòú). Instead, a HMM has *emissions*. Each state $s$ is associated to a distribution $\\mathcal{D}_s$. So when the system is in state $s=1$, we will observe values $x$ distributed according to $\\mathcal{D}_1$, etc ... The correspondance between each state $s$ and its distribution $\\mathcal{D}_s$ is stored in the emission matrix $E_s$.\n",
    "\n",
    "**In our case :**\n",
    "- we still want to model the re-orientation dynamics with the 3 states 'Forward', 'Left', and 'Right'.  \n",
    "- what we observe are the re-orientations angles $\\delta \\theta$, and we've seen in section 2 that we can approximate them by 3 Normal distributions. Hence our 3 emissions will be $\\mathcal{N}(\\mu_F, \\sigma_F)$, $\\mathcal{N}(\\mu_L, \\sigma_L)$, and $\\mathcal{N}(\\mu_R, \\sigma_R)$. \n",
    "\n",
    "As all emissions are gaussian, our HMM will be a subclass of HMMs called a GaussianHMM. The emission matrix is therefore simplified to a vector $\\mu_s = [\\mu_F, \\mu_L, \\mu_R]$ and a vector $\\sigma_s = [\\sigma_F, \\sigma_L, \\sigma_R]$, which together describe each emission distributions.\n",
    "\n",
    "![HMM](https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/img/HMM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b75386-a5b2-4bf4-b951-54552f97ea7f",
   "metadata": {},
   "source": [
    "First let's reload the data for 26¬∞C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d404a9-3b23-40da-9a87-4543c9402ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = file[\"/behaviour/26/\"]\n",
    "xs, ys = data[\"xpos\"][:], data[\"ypos\"]  # x and y positions \n",
    "dthetas = data[\"dtheta\"][:]             # reorientation angles\n",
    "dts = data[\"interboutintervals\"][:]     # interbout intervals\n",
    "distance = data[\"displacements\"][:]     # distance traveled with each bout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd7260-411f-43ef-8710-1755a4ebf715",
   "metadata": {},
   "source": [
    "## 3.1 Preparing the data\n",
    "\n",
    "We will use the library [hmmlearn](https://hmmlearn.readthedocs.io/en/latest/) (imported at the top of this notebook as `hmm`) instead of programming our own HMMs.  \n",
    "However for this, we first need to prepare our data in a format which will be understood by `hmm`.\n",
    "\n",
    "The first step if to remove all the NaNs from $\\delta \\theta$ sequences, because `hmm` cannot work with NaNs.  \n",
    "**‚û°Ô∏èüë§‚å®Ô∏è Make a list called `sequences` containing all the $\\delta \\theta$ sequences in `dthetas`, but without the NaNs. And make a second list called `lenghts` containing the lenght (aka. number of bouts) for each sequence in `sequences`. ‚û°Ô∏èüë§‚å®Ô∏è**  \n",
    "\n",
    "üí° You can use the function `np.isfinite` to identify where an array is not equal to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f3ed8-b79b-4fcb-b493-d2a18e136acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "sequences = ...\n",
    "lenghts = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a838f8-f7d9-473e-9234-064331d16465",
   "metadata": {},
   "source": [
    "‚û°Ô∏èüë§‚å®Ô∏è Now **stitch all the sequences in `sequences` together as one 1D vector (this is called concatenation).** ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "\n",
    "üí° You can use the function `np.concatenate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928c1e6-c23e-4353-89f9-39248edb91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "long_seq = ...\n",
    "long_seq.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc90f66-2870-40ca-b2ad-0dd1514b628b",
   "metadata": {},
   "source": [
    "## 3.2 Preparing the Model\n",
    "\n",
    "Now we can build the HMM.  \n",
    "We will provide the HMM with an initial guess of its parameters :\n",
    "- the transition matrix $T_cn = P(c‚Üín)$ between hidden states we computed from the MC : `T_bouts`.\n",
    "- the *a priori* probability of each hidden state $P(s)$ we computed from the MC : `p_bouts`.\n",
    "- gaussian emission means $\\mu_s = [\\mu_F, \\mu_L, \\mu_R] = [0,-30,+30]$.\n",
    "- gaussian emission variance $\\sigma_s^2 = [\\sigma_F^2, \\sigma_L^2, \\sigma_R^2] = [11,700,700]$.\n",
    "\n",
    "‚ùó If you had time to do section 2 on Gaussian Mixture Models, you can use the parameters of the 3 subpopulations you found for $\\mu_s$ and $\\sigma_s^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615df98-55cb-4190-8ab4-777dd801753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hmm.GaussianHMM(\n",
    "    n_components=3,                # the number of hidden states we want\n",
    "    covariance_type=\"spherical\",   # this is complicated, you can look at the doc if you want to understand\n",
    "    init_params=\"\",                # this is complicated, you can look at the doc if you want to understand\n",
    "    algorithm='viterbi',           # the decoder algorithm\n",
    ")\n",
    "\n",
    "model.transmat_ = T_bouts\n",
    "model.startprob_ = p_bouts\n",
    "model.means_ = np.array([[0], [-30], [+30]])\n",
    "model.covars_ = np.array([[11],[700],[700]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eee5b8-24bc-41e3-9997-bd7eeefd49fc",
   "metadata": {},
   "source": [
    "## 3.3 Investigating the model\n",
    "\n",
    "Now that our model is initiated, we can look at it in more detail.\n",
    "\n",
    "‚ùó Remember that for now, our model still hasn't been trained. Its parameters are guesses.\n",
    "\n",
    "First let's generate a very long sequence of bouts, and look at the distribution of $\\delta \\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356136f3-68af-4f13-8fc8-d516d030c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dthetas, gen_states = model.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5db86-36b8-436d-81da-449da48ff50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_,bins,_ = ax.hist(dthetas.ravel(), bins=100, color=\"k\", density=True, label=\"Data\");\n",
    "ax.hist(\n",
    "    gen_dthetas.ravel(), bins=bins, \n",
    "    color=\"orange\", density=True, \n",
    "    histtype=\"step\", label=\"Generated\",\n",
    "    linewidth=2,\n",
    ");\n",
    "ax.set_xlabel(r\"$\\delta \\theta$ (degree)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e973236-1810-4fb3-a9ec-be1953f11261",
   "metadata": {},
   "source": [
    "Are the data and generated distributions the same ? Why do you think that is ?\n",
    "\n",
    "Let's look at the transition matrix and *a priori* state proabilities : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc86ddc-4297-4061-bcf9-35720f35ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(3.5*5,5))\n",
    "plot_transition_matrix(axs[0], T_bouts)\n",
    "axs[0].set_title(\"Data\")\n",
    "plot_transition_matrix(axs[1], bout_transition([gen_states]))\n",
    "axs[1].set_title(\"Generated\")\n",
    "ax = axs[2]\n",
    "width = 0.3\n",
    "x = np.arange(3)\n",
    "ax.bar(x-width/2, p_bouts, width, label=\"Data\", color=\"k\")\n",
    "ax.bar(x+width/2, bout_proba(gen_states), width, label=\"Generated\", color=\"orange\")\n",
    "ax.legend()\n",
    "ax.set_xticks([0,1,2], [\"forward\", \"left\", \"right\"])\n",
    "ax.set_ylabel(\"Bout Probability\")\n",
    "ax.set_title(r\"P(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f11065-c31b-4f07-9718-c10a94082ac5",
   "metadata": {},
   "source": [
    "Again, are data and generated the same ? Why do you think that is ?\n",
    "\n",
    "We can also look at the Stationary Distribution, $\\mu_s$ and $\\sigma_s^2$, which at this stage should not be surprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c86c7-13c2-4321-87d2-a5e324a3f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_stationary_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa26a9a-c82a-4428-b908-3f7e342e59e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158e2ec-4f1c-4bcd-a05f-c3228de507ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.covars_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c80771f-daa9-4559-bcc3-b95a45e1be90",
   "metadata": {},
   "source": [
    "Similarly to what we did in section 2, we can ask the model to predict the hidden state given a observation.  \n",
    "\n",
    "‚û°Ô∏èüë§‚å®Ô∏è **Predict the states for one of the sequences in `sequences`.** ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "\n",
    "üí° you can use the function `model.predict`.  \n",
    "üí° `model.predict` expects a 2D array, you can use `np.newaxis` to solve that problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb314539-90d7-4a11-a2ef-ad2880d781ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "sequence = sequences[1]\n",
    "predicted_states = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea219d10-0179-41b9-9d8c-744643faa81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "plot_angle_sequence(ax, sequence, states=predicted_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784dce2a-a544-42a9-953b-95f15131e640",
   "metadata": {},
   "source": [
    "Under the hood, `model.predict` predicts the state $s_i$ of observation $\\delta \\theta_i$, by computing the probability that $s_i$ is observed in each of the 3 hidden states : $P(\\delta \\theta_i | F)$, $P(\\delta \\theta_i | L)$ and $P(\\delta \\theta_i | R)$. This is done using the [Viterbi Algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm). The state for which this probability is maximum becomes the predicted state : $$s_i = \\max_{s\\in {F,L,R}} P(\\delta \\theta_i | s)$$\n",
    "\n",
    "‚û°Ô∏èüë§‚å®Ô∏è **Find the probability that each observation in `sequence` is in state 'Forward'.** ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "\n",
    "üí° you can use the function `model.predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fafe1-0c6a-451e-8a07-802da0f2e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "predicted_forward = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762b8f8-7f22-4f98-af25-2f0adf840e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "plot_angle_sequence(ax, sequence, probas=predicted_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf53502-789d-4748-a5c0-e0217a6cf79e",
   "metadata": {},
   "source": [
    "## 3.4 Generating sequences\n",
    "\n",
    "As with Markov Chains, we can generate sequences of states by sampling the transition matrix. The difference with HMMs is that we can also generate observed emissions.\n",
    "\n",
    "In this section we will investigate how realistic are the re-orientations sequences generated by the HMM.  \n",
    "\n",
    "‚û°Ô∏èüë§‚å®Ô∏è **Generate sequences of $\\delta \\theta$s with the following constraints :** ‚û°Ô∏èüë§‚å®Ô∏è \n",
    "- you should generate the same number of sequences as are found in `sequences`\n",
    "- the generated sequences should have lenghts distributed similarly to the ones found in `sequences`\n",
    "- store the generated $\\delta \\theta$s and state sequences in lists, similarly to `sequences`\n",
    "\n",
    "üí° to generate you can use the function `model.sample`  \n",
    "‚ö†Ô∏è `model.sample` will output a 2D array. You only want a 1D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf9c4e-9bdf-4d98-84e2-011c752e0f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "gen_dthetas, gen_states = [],[]\n",
    "for _ in tqdm(sequences):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263fb441-5877-467c-acd7-fa05e31212ff",
   "metadata": {},
   "source": [
    "Let's look at one of the generated sequence. (change `i` to look at different sequences)\n",
    "Does it look similar to the orignal data ?  \n",
    "According to you, are the states classified correctly ? Why do you think that is ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07feec4-4c5e-4841-b1bb-2eaaffcb7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "plot_angle_sequence(ax, gen_dthetas[i], states=gen_states[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f520f0-7199-4638-bca9-6b2866990818",
   "metadata": {},
   "source": [
    "To make it a bit more intuitive, we can reconstruct fake $(x,y)$ larvae trajectories from the generated $\\delta \\theta$ and by sampling from the `distance` distribution (aka. how far the larva moves during each bout). This will not recreate perfectly realistic trajectories, but will provide a rough estimate.\n",
    "\n",
    "‚ùó If you have enough time, and feel up to the challenge, you can try to code the function yourself. But warning! , it is a bit hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d53c45-d1c2-4f80-89ff-65196e417cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_trajs(dthets):\n",
    "    \"\"\"Reconstruct (x,y) trajectory from a sequencere-orientation angles.\n",
    "    \n",
    "    Parameters :\n",
    "    ------------\n",
    "    dthets : 1d array\n",
    "        sequence of re-orientation angles\n",
    "\n",
    "    Return :\n",
    "    --------\n",
    "    x : 1d array\n",
    "        reconstructed x positions\n",
    "    y : 1d array\n",
    "        reconstructed y positions\n",
    "    \"\"\"\n",
    "    angles = np.cumsum(-np.deg2rad(dthets))\n",
    "    dists = np.random.choice(distance[np.isfinite(distance)], size=len(dthets))\n",
    "    dxy =  dists[:,np.newaxis]* np.c_[np.cos(angles), np.sin(angles)]\n",
    "    x,y = tuple(np.cumsum(dxy, axis=0).T)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b5293-2d54-4be4-916a-1f312e2d28d1",
   "metadata": {},
   "source": [
    "Let's apply it to a generated trajectory.  \n",
    "‚ùó you can run the next cell a few times, with the same sequence `i`, and different sequences, to build a better intuition.\n",
    "\n",
    "Does this trajectory look relalistic to you ?  \n",
    "Does it look like the other trajectories at 26¬∞C ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9126d-3979-418d-b65a-2d6d54b03c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "gen_x, gen_y = reconstruct_trajs(gen_dthetas[i])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,9))\n",
    "ax.plot(gen_x,gen_y)\n",
    "h = ax.scatter(\n",
    "    gen_x, \n",
    "    gen_y, \n",
    "    c=np.insert(gen_states[i][1:], -1,0), \n",
    "    cmap=cmap_states, edgecolors=\"k\", s=100\n",
    ")\n",
    "ax.scatter(gen_x[0], gen_y[0], marker=4, s=500, color=\"k\")\n",
    "cbar = fig.colorbar(h, ax=ax)\n",
    "cbar.set_label(\"States\")\n",
    "cbar.set_ticks([0.33, 1, 1.66], labels=[\"Forward\", \"Left\", \"Right\"])\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351fca0b-2d5a-420f-8cb0-50665b940e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "030051c1-c3ad-407a-ac43-a76ef80b8367",
   "metadata": {},
   "source": [
    "## 3.5 Mean Square Reorientation\n",
    "\n",
    "As we have said before, Markov Chains (and therefore Hidden Markov Models) are memory-less systems. The next step only depends on the current step. Therefore they only model very short-term dynamics, and might completly miss the long-term dynamics present in the data. In fact, if a system can be perfectly modeled with a MC or HMM, then it is a very strong clue that this system has no memory, and we call them *markovian*.\n",
    "\n",
    "But how do we check if our HMM model reproduces long-term dynamics of re-orientations ?\n",
    "\n",
    "One thing we can look at is the **Mean Square Reorientation** (MSR).  \n",
    "MSR is a measurment of how much your direction changes compared to your initial direction, but taking the initial direction anywhere on your trajectory. Let's say that again but differently. You will have $M_q$ the measurement of how much your angle has changes on average when you make $q$ bouts (this is called the lag). This is calculated by averaging over all bouts in the trajectory.\n",
    "$$\\begin{split}\n",
    "M_q &= \\Biggl \\langle  (\\theta_{n+q} - \\theta_{n})^2  \\Biggr \\rangle_{n}\\\\\n",
    "    &= \\Biggl \\langle  \\left(\\sum_{i=0}^{q-1} \\delta \\theta_{n+i} \\right)^2  \\Biggr \\rangle_{n}\\\\\n",
    "    &= \\frac{1}{N-q+1} \\sum_{n=0}^{N-q} \\left(\\sum_{i=0}^{q-1} \\delta \\theta_{n+i} \\right)^2\n",
    "\\end{split}$$\n",
    "with $N$ the number of bouts in a trajectory.\n",
    "\n",
    "‚û°Ô∏èüë§‚å®Ô∏è **Implement this function $M_q$.** ‚û°Ô∏èüë§‚å®Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f675d0-6120-45dd-a83e-cf4e9457d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSR_q(seq, q):\n",
    "    \"\"\"Compute the Mean Squared Reorientation of a sequence.\n",
    "    \n",
    "    Parameters : \n",
    "    ------------\n",
    "    seq : 1D array of floats\n",
    "        sequences of re-orientations angles.\n",
    "    q : integer\n",
    "        the lag.\n",
    "\n",
    "    Return :\n",
    "    --------\n",
    "    m_q : float\n",
    "        the computed M_q (see math function above)\n",
    "    \"\"\"\n",
    "    assert seq.ndim==1, \":seq: should be a 1D vector.\"\n",
    "\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "    ...\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "\n",
    "    assert \"float\" in str(type(m_q)), \":m_q: should be a float.\"\n",
    "    return m_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98a1ce-b4bd-42a1-916c-443db2af5939",
   "metadata": {},
   "source": [
    "Now, when we look at MSR, we are interested in more than just 1 value of lag $q$.  \n",
    "‚û°Ô∏èüë§‚å®Ô∏è **Write a function to compute the MSR for multiple values of $q$.** ‚û°Ô∏èüë§‚å®Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7dedc9-bd08-4529-b646-7486f25694e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSR(seq, qs):\n",
    "    \"\"\"Compute the MSR of a sequence at multiple lags.\n",
    "    \n",
    "    Parameters : \n",
    "    ------------\n",
    "    seq : 1D array of floats\n",
    "        sequences of re-orientations angles.\n",
    "    qs : 1D array of integers\n",
    "        the lags.\n",
    "\n",
    "    Return :\n",
    "    --------\n",
    "    m_qs : 1D array\n",
    "        the computed M_q's (see math function above)\n",
    "    \"\"\"\n",
    "    assert seq.ndim==1, \":seq: should be a 1D vector.\"\n",
    "\n",
    "    m_qs = np.empty_like(qs, dtype=np.float_)\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "    ...\n",
    "    # ‚û°Ô∏èüë§‚å®Ô∏è\n",
    "\n",
    "    assert m_qs.ndim==1, \":m_qs: should be a 1D vector.\"\n",
    "    assert len(m_qs)==len(qs), \":m_qs: should have the same lenght as :qs:.\"\n",
    "    return m_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a764ef2f-8f7c-4288-86b2-1c1facc5f77d",
   "metadata": {},
   "source": [
    "Now lets compute MSR for all sequences, over large range of lags $q \\in (0,15)$.  \n",
    "We will then compute the mean MSR over the sequences, and the standard error of the mean. And then plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ea2ae-aebb-4657-8a50-9f0ee95ad1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.arange(0,15)\n",
    "MSRS = np.empty((len(sequences),len(qs)), dtype=np.float_)\n",
    "for i in tqdm(range(len(sequences))):\n",
    "    MSRS[i] = MSR(sequences[i], qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06cbab-47f5-41a0-a30b-0b958b6fb766",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSR_mean = np.nanmean(MSRS, axis=0) \n",
    "MSR_std = np.nanstd(MSRS, axis=0)/ np.sqrt(MSRS.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e19a0-70fe-4126-a374-798509dc1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(qs, MSR_mean, color=\"k\", label=\"mean\")\n",
    "ax.fill_between(qs, MSR_mean-MSR_std, MSR_mean+MSR_std, alpha=0.4, color=\"k\", label=\"st. err. of the mean\")\n",
    "ax.set_xlabel(\"Lag $q$\")\n",
    "ax.set_ylabel(\"MSR\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de66a62-0653-4d12-9931-69a148870960",
   "metadata": {},
   "source": [
    "This can be a bit hard to interpret if you are not used to MSR curve. So lets find something to compare this curve with.\n",
    "\n",
    "Imagine if there was no structure in the reorientation angles, and the larvae would choose randomely the $\\delta \\theta$. What would that look like ?\n",
    "\n",
    "To generate this fake/random sequences, we will just choose random re-orientation from the distribution of $\\delta \\theta$s, this way, the probability of each $\\delta \\theta$ will stay the same but we will loose all the temporal structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be484bf-f136-4cb1-a668-4f314c8bdc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seq = np.random.choice(long_seq, size=(len(sequences),np.mean(lenghts).astype(int)))\n",
    "MSR_rand = np.empty((len(sequences),len(qs)), dtype=np.float_)\n",
    "for i in tqdm(range(len(sequences))):\n",
    "    MSR_rand[i] = MSR(rand_seq[i], qs)\n",
    "MSR_rand_mean = np.nanmean(MSR_rand, axis=0) \n",
    "MSR_rand_std = np.nanstd(MSR_rand, axis=0)/ np.sqrt(MSR_rand.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74f92d-879c-42cc-bb6a-12df2b98bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "a_dat = ax.plot(qs, MSR_mean, color=\"k\")\n",
    "h_dat = ax.fill_between(qs, MSR_mean-MSR_std, MSR_mean+MSR_std, alpha=0.4, color=\"k\")\n",
    "ax.plot(qs, MSR_rand_mean, color=\"red\")\n",
    "h_rand = ax.fill_between(qs, MSR_rand_mean-MSR_rand_std, MSR_rand_mean+MSR_rand_std, alpha=0.4, color=\"red\")\n",
    "\n",
    "ax.set_xlabel(\"Lag $q$\")\n",
    "ax.set_ylabel(\"MSR\")\n",
    "leg1 = plt.legend([a_dat[0], h_dat], [\"mean\", \"st. err. of the mean\"], loc=\"upper left\")\n",
    "leg2 = plt.legend([h_dat, h_rand], [\"data\", \"random\"], loc=\"lower right\")\n",
    "ax.add_artist(leg1)\n",
    "ax.add_artist(leg2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579d71a-4d50-4675-82d6-35a08567308c",
   "metadata": {},
   "source": [
    "How can you interpret this graph ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d74572-e2a9-4189-90eb-317dd11f5634",
   "metadata": {},
   "source": [
    "Now let's build the MSR curve for the data generated from the HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346abcb6-6250-40fb-af8e-44c636069369",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSR_hmm = np.empty((len(sequences),len(qs)), dtype=np.float_)\n",
    "for i in tqdm(range(len(sequences))):\n",
    "    MSR_hmm[i] = MSR(gen_dthetas[i], qs)\n",
    "MSR_hmm_mean = np.nanmean(MSR_hmm, axis=0) \n",
    "MSR_hmm_std = np.nanstd(MSR_hmm, axis=0)/ np.sqrt(MSR_hmm.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ff0bc-6eed-4c21-9037-3e1f1691b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "a_dat = ax.plot(qs, MSR_mean, color=\"k\")\n",
    "h_dat = ax.fill_between(qs, MSR_mean-MSR_std, MSR_mean+MSR_std, alpha=0.4, color=\"k\")\n",
    "ax.plot(qs, MSR_rand_mean, color=\"red\")\n",
    "h_rand = ax.fill_between(qs, MSR_rand_mean-MSR_rand_std, MSR_rand_mean+MSR_rand_std, alpha=0.4, color=\"red\")\n",
    "ax.plot(qs, MSR_hmm_mean, color=\"blue\")\n",
    "h_hmm = ax.fill_between(qs, MSR_hmm_mean-MSR_hmm_std, MSR_hmm_mean+MSR_hmm_std, alpha=0.4, color=\"blue\")\n",
    "\n",
    "ax.set_xlabel(\"Lag $q$\")\n",
    "ax.set_ylabel(\"MSR\")\n",
    "leg1 = plt.legend([a_dat[0], h_dat], [\"mean\", \"st. err. of the mean\"], loc=\"upper left\")\n",
    "leg2 = plt.legend([h_dat, h_rand, h_hmm], [\"data\", \"random\", \"hmm\"], loc=\"lower right\")\n",
    "ax.add_artist(leg1)\n",
    "ax.add_artist(leg2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46390bb-7cec-4c3b-89b3-a5f362e1016f",
   "metadata": {},
   "source": [
    "What can you conclude ? Is the HMM better than random ? Equivalent to the data ? If not, can you interpret what is the difference ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90158ad-e366-486d-a320-96c438ae8cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4874b081-c707-4c75-86dc-c0238af6b3ba",
   "metadata": {},
   "source": [
    "## 3.6 Training HMMs\n",
    "\n",
    "Until now we have worked with an HMM intitialised with parameters which were only guesses.  \n",
    "However, most of the time, we prefer to fit them on the data, starting from an initial guess.\n",
    "\n",
    "This can be done very easily with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08909b-5cee-4f0b-af9e-a3923660f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(long_seq[:,np.newaxis], lenghts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c6e0a-1f4f-44fb-9ec6-a3781e4ae367",
   "metadata": {},
   "source": [
    "By learning directly from the data, the fiting algorithm modified the $T_{cn}$, $\\mu_s$, and $\\sigma_s^2$.  \n",
    "‚û°Ô∏èüë§‚å®Ô∏è **Re-run the cells in sections 3.3 to 3.5 to see what changed.** ‚û°Ô∏èüë§‚å®Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229baf75-dd22-4ed4-8841-46a81af7687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "a_dat = ax.plot(qs, MSR_mean, color=\"k\")\n",
    "h_dat = ax.fill_between(qs, MSR_mean-MSR_std, MSR_mean+MSR_std, alpha=0.4, color=\"k\")\n",
    "ax.plot(qs, MSR_rand_mean, color=\"red\")\n",
    "h_rand = ax.fill_between(qs, MSR_rand_mean-MSR_rand_std, MSR_rand_mean+MSR_rand_std, alpha=0.4, color=\"red\")\n",
    "ax.plot(qs, MSR_hmm_mean, color=\"blue\")\n",
    "h_hmm = ax.fill_between(qs, MSR_hmm_mean-MSR_hmm_std, MSR_hmm_mean+MSR_hmm_std, alpha=0.4, color=\"blue\")\n",
    "\n",
    "ax.set_xlabel(\"Lag $q$\")\n",
    "ax.set_ylabel(\"MSR\")\n",
    "leg1 = plt.legend([a_dat[0], h_dat], [\"mean\", \"st. err. of the mean\"], loc=\"upper left\")\n",
    "leg2 = plt.legend([h_dat, h_rand, h_hmm], [\"data\", \"random\", \"hmm\"], loc=\"lower right\")\n",
    "ax.add_artist(leg1)\n",
    "ax.add_artist(leg2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3433fb-bcdd-4d7b-bf22-3763ab5fca38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b003a-c015-48e7-a140-2ae58165bd73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1003863-2967-41dd-aaec-a81c15e9259b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
