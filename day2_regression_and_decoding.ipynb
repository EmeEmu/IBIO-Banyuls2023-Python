{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll try to understand brain activity relates to external signals, by analysing the results of an experiment where a fish was exposed to a visual stimulus while we recorded its brain activity and tail movements.\n",
    "As a reminder this is a schematic of the experimental setup:\n",
    "\n",
    "![setup](https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/img/experimental_setup_danionella.png)\n",
    "\n",
    "We'll start by looking at correlations with a single external signal, then we'll use multiple linear regression to see how different signals are encoded in the brain and finally we'll try to decode external signals from brain activity.\n",
    "\n",
    "Some parts are missing and you'll have to code them yourself, they are highlighted with a keyboard symbol ⌨️."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started by importing the necessary libraries and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import libraries and dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data and helper functions in the workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /content/Helper_Functions/\n",
    "!wget -P /content/Helper_Functions/ https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/Helper_Functions/accessing_data.py\n",
    "!wget -P /content/Helper_Functions/ https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/Helper_Functions/hmm_plotters.py\n",
    "!wget -P /content/Helper_Functions/ https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/Helper_Functions/OrthoViewer.py\n",
    "!wget -P /content/Helper_Functions/ https://raw.githubusercontent.com/EmeEmu/IBIO-Banyuls2023-Python/main/Helper_Functions/plotting_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --folder 1k21VhLoonOnoxxXyswrmE45VIB4FF00n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from Helper_Functions.accessing_data import h5tree_view\n",
    "from ipywidgets import interact\n",
    "from scipy.signal import convolve\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import scale\n",
    "from Helper_Functions.OrthoViewer import OrthoAxes\n",
    "from Helper_Functions.plotting_functions import plot_dff_traces,plot_dff_raster,plot_brain_layers,plot_brain_projections,plot_coefficients,plot_neurons_per_label\n",
    "from sklearn.linear_model import LinearRegression,Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install ipympl to activate interactive plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipympl\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set resolution of the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi']=150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically reload modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the file `fish1_different_speeds.hdf5`, visualize the data tree and save some arrays to local variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d59tQmCWmoK1"
   },
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "brain_times=\n",
    "dff=\n",
    "coords=\n",
    "stimulus_speed=\n",
    "stimulus_times=\n",
    "stimulus_T=\n",
    "tail_times=\n",
    "deflection=\n",
    "forward_thrust=\n",
    "side_thrust="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to close the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUEl8vy9moK8"
   },
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Correlation between neural activity and external signals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the signal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we'll try to find neurons whose activity is correlated with tail movements.  Even though the fish is head-immobilized, it can move its tail and we can calculate the strength of such movements.  `forward_thrust` is one such a measure, the higher the value the stronger the force the fish is exerting to try and move forward.\n",
    "\n",
    "We're going to look for neurons whose activity is correlated with this signal, but first we have to do some preprocessing in order to make it comparable with the fluorescence time series.\n",
    "\n",
    "We start by plotting `forward_thrust` together with `deflection`, a measure of the deflection of the tail from its resting position:\n",
    "\n",
    "In order to make the two signals comparable, you can use the function `scale` from sklearn to normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we see that `forward_thrust` is zero when the tail is at rest and it becomes positive when the tail oscillates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fluorescence that we observe is a indicator of neural activity, it's a measure of the concentration of calcium ions inside the neuron, which in turn is correlated with its firing rate.  Because of the unbinding kinetics of the calcium indicator, the fluorescence decays exponentially following an action potential with a characteristic time of $\\tau \\simeq 3$ s.\n",
    "\n",
    "Fluorescence traces can be seen as a convolution of the spike train with a calcium impulse response function, which we'll take to be a simple exponential.\n",
    "\n",
    "Because of this, if we want to compare our behavioral signal with the fluorescence traces, we'll have to convolve it with an exponential kernel of the form: $H(t) e^{-t/\\tau}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the exponential kernel for the convolution, note that $t=0$ has to corresponds to the center of the array and the spacing between two points needs to be the same as for the tail signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "tau=3 #characteristic decay time\n",
    "kernel_size=10 #size of half of the kernel in units of tau\n",
    "dt= #spacing between successive timepoints\n",
    "n_points= #number of timepoints in half of te kernel\n",
    "kernel_times=np.linspace() #linearly spaced array from -n_points*dt to n_points*dt with spacing dt\n",
    "kernel= #define kernel as exponential decay\n",
    "kernel[]=0 #set to zero for negative times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it to check that we implemented it properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.plot(kernel_times,kernel)\n",
    "ax.set_xlabel('time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform the convolution, we can use the function `convolve` from scipy, which perform a discrete convolution, we're trying to approximate a continuous convolution so we have to multiply it by the time increment in between two points: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "forward_thrust_conv="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the original signal together with the convolved one:: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can in principle compare this signal with the neural activity, but if you look at the shape of `forward_thrust_conv` and `dff` you'll notice that there are many more timepoints in the tail signal. This is because the tail images were acquired at a much higher frame rate.  In order to compare the to time series we have to downsample the tail signal so that it is evaluated at the same times as the neural activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so we'll perform a linear interpolation which basically means connecting the dots in our plot with straight lines (something that matplotlib already does by default when we're visualizing it). You can use the function `interp1d` from scipy to perform the interpolation, it returns a function that you can then evaluate on the times at which the fluorescence was measured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "forward_thrust_conv_interp_f= #perform the interpolation\n",
    "signal_forward= #get the downsampled signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results by plotting the original signal together with the downsampled one (change the marker option in the plotting function to see the single unconnected datapoints):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function that does all this in once to make this preprocessing easier in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_and_interpolate(signal,signal_times,output_times,tau=3,kernel_size=10):\n",
    "    \"\"\"Convolve signal with the calcium impulse response function and resample it at different times using linear interpolation.\n",
    "\n",
    "    Parameters :\n",
    "    ------------\n",
    "    signal : 1d array\n",
    "        values of the time series\n",
    "        \n",
    "    signal_times : 1d array\n",
    "        timepoints of the time series\n",
    "\n",
    "    output_times : 1d array\n",
    "        timepoints at which we want to resample the signal\n",
    "    \n",
    "    tau : integer\n",
    "        characteristic decay time of the exponential kernel in seconds\n",
    "        default : 3\n",
    "    \n",
    "    kernel_size : integer\n",
    "        size of the kernel in units of tau\n",
    "        default : 10  \n",
    "        \n",
    "    Return :\n",
    "    --------\n",
    "    signal_conv_interp : 1d array\n",
    "        resampled values of the convolved time series\n",
    "    \"\"\"\n",
    "    #⌨️⬇️\n",
    "\n",
    "    \n",
    "    return signal_conv_interp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating correlations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNqGmp85VfWf"
   },
   "source": [
    "As a measure of correlation we can calculate the covariance between the preprocessed swimming strength and the neural fluorescence traces.  \n",
    "\n",
    "$$Cov(x,y)=\\overline{(x-\\bar{x})\\cdot(y-\\bar{y})}$$\n",
    "\n",
    "Regions where both signals are larger or smaller than their averages give positive contibutions, whereas regions where one is larger and the other is smaller give negative contributions.  This way correlated signals we'll have a positive covariance, anticorrelated signals a negative covariance, and uncorrelated signals a covariance that is close to zero (how much?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that gives us the covariance values between some signal and every single neural activity trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_covariance(signal,dff):\n",
    "    \"\"\"Calculate covariance values between signal and rescaled fluorescence for all neurons.\n",
    "\n",
    "    Parameters :\n",
    "    ------------\n",
    "    signal : 1d array\n",
    "        external signal, shape (n_timepoints)\n",
    "        \n",
    "    dff : 2d array\n",
    "        rescaled fluorescence traces for all neurons, shape (n_neurons,n_timepoints)\n",
    "        \n",
    "    Return :\n",
    "    --------\n",
    "    covariance : 1d array\n",
    "        covariance values\n",
    "    \"\"\"\n",
    "    #⌨️⬇️\n",
    "    n_neurons=len(dff)\n",
    "    signal_centered= #subtract the average from the signal\n",
    "    dff_centered= #subtract the average from every fluorescence trace\n",
    "    covariance= #initialize the covariance\n",
    "    for i in range(n_neurons): #loop over all neurons\n",
    "        covariance[i]= #evaluate the covariance\n",
    "    \n",
    "    return covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the covariances with the swimming strength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6VL4M2nVfWf"
   },
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "covariance_forward="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the results by plotting a histogram of the covariance values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "salpeIdwVfWg",
    "outputId": "8aaf6e40-3102-47b1-96dd-d9721ce0078c"
   },
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating statistical significance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know if a covariance value is significantly larger than zero?\n",
    "We need a way to define a threshold above which covariances are statistically significant.\n",
    "\n",
    "We would like to know the distribution of covariances in the case where the two signals are uncorrelated.\n",
    "We can estimate it with a method called bootstrapping, it consists in resampling our signal in order to create new signals with the same statistics.\n",
    "As we're working with time series, adjacent samples are not independent but correlated in time, thus we're going to resample blocks of consecutive samples in order to preserve the structure on short times.\n",
    "\n",
    "We choose a method called stationary bootstrap, in which the size of the blocks is randomized as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's might be a bit complicated, you can try to code it yourself but if you don't manage just have a look at it and try understand how it works.  We want to create a new signal by sampling blocks of the original signal with a length that is sampled from a geometric distribution.  In practice this is equivalent to choosing as first value of our new array a random value of the original array and then for each successive value we either take the successive value in the original array with probability $1-p$, or a new random value with probability $p$.  If we reach the end of the original array then we consider the first value as the succesive one.  $p$ is a parameter of the geometric distribution and it turns out that it's the inverse of the average size of the blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_bootstrap(signal,average_block_size=240):\n",
    "    \"\"\"resample a time series using stationary bootstrap, it samples with replacement random blocks of sizes following a geometric distribution.\n",
    "\n",
    "    Parameters :\n",
    "    ------------\n",
    "    signal : array\n",
    "        original time series, the first dimension corresponds to time, shape (n_timepoints,...)\n",
    "        \n",
    "    average_block_size : integer\n",
    "        average number of values in a block during resampling\n",
    "        default : 240\n",
    "        \n",
    "    Return :\n",
    "    --------\n",
    "    resampled_signal : array\n",
    "        bootstrapped time series\n",
    "    \"\"\"\n",
    "    #⌨️⬇️\n",
    "    l=len(signal)\n",
    "    p= #probability of choosing a new random value \n",
    "    resampled_signal= #initialize the output array\n",
    "    j= #index of the first random value \n",
    "    resampled_signal[0]= #assign the first random value\n",
    "    for i in range(1,l): #loop over the next positions in the new array\n",
    "        if : #sample a random number to choose in between the two options\n",
    "            j= #take the next index\n",
    "        else:\n",
    "            j= #take a random index\n",
    "        resampled_signal[i]= #assign the value to the new array\n",
    "    \n",
    "    return resampled_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at some resulting bootstrapped signals, try and tune the average duration of the blocks and see if you obtain the expected results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a null distribution of covariances using the bootstrapped signals, we expect the covariance to follow this distribution for neural activity that is independent of the behavioral signal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define a function that creates some bootstrapped signals and calculates their covariances with the neural activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_covariance_bootstrap(signal,dff,n_resamples=50,average_block_size=240):\n",
    "    \"\"\"Calculate covariance values between resampled signal using stationary bootstrap and rescaled fluorescence for all neurons.\n",
    "\n",
    "    Parameters :\n",
    "    ------------\n",
    "    signal : 1d array\n",
    "        external signal, shape (n_timepoints)\n",
    "        \n",
    "    dff : 2d array\n",
    "        rescaled fluorescence traces for all neurons, shape (n_neurons,n_timepoints)\n",
    "    \n",
    "    n_resamples: integer\n",
    "        number of resampled signals used for calculating covariance\n",
    "        default : 50\n",
    "        \n",
    "    average_block_size : integer\n",
    "        average number of values in a block during resampling\n",
    "        default : 240\n",
    "        \n",
    "    Return :\n",
    "    --------\n",
    "    covariance_bootstrap : 1d array\n",
    "        bootstrapped covariance values, shape (n_resamples*n_neurons)\n",
    "    \"\"\"\n",
    "    #⌨️⬇️\n",
    "    n_neurons=len(dff)\n",
    "    covariance_bootstrap= #initialize the output array\n",
    "    \n",
    "    for i in tqdm(range(n_resamples)):\n",
    "        signal_resampled= #sample a bootstrapped time series\n",
    "        covariance_bootstrap[i*n_neurons:(i+1)*n_neurons]= #calculate covariances with neural traces\n",
    "        \n",
    "\n",
    "        \n",
    "    return covariance_bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this function to get the covariance values and compare their distribution with the one of the covariances with the original signal by plotting them together.\n",
    "You'll have to use the `density` parameter while plotting in order to normalize the two histograms and make them comparable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "covariance_forward_bootstrap="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "salpeIdwVfWg",
    "outputId": "8aaf6e40-3102-47b1-96dd-d9721ce0078c"
   },
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say by looking at the two distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an estimate of the null distribution, we can calculate the p-value associated to a certain covariance, it's the probability of obtaining an even larger value of covariance according to the null distribution.\n",
    "\n",
    "We first have to create the empirical distribution function with the boostrapped covariances to approximate the null distribution, then we can evaluate it on the original covariances to calculate the corresponding p-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pvalue(statistic,statistic_bootstrap):\n",
    "    \"\"\"Calculate p-values for a set of statistics, based on a null distribution estimated with a set of bootstrapped statistics.\n",
    "\n",
    "    Parameters :\n",
    "    ------------\n",
    "    statistic : 1d array\n",
    "        values of a statistic for which we want to calculate the p-value\n",
    "        \n",
    "    statistic_bootstrap : 1d array\n",
    "        bootstrapped values of a statistic from which we estimated the null distribution\n",
    "        \n",
    "    Return :\n",
    "    --------\n",
    "    pvalue : 1d array\n",
    "        p-values\n",
    "    \"\"\"\n",
    "    #⌨️⬇️\n",
    "    statistic_bootstrap= #flatten array in case it's not already one-dimensional\n",
    "    statistic_bootstrap_sorted= #sort the values in increasing order\n",
    "\n",
    "    probability_of_smaller_values= #express the probability of finding a smaller value of the statistic\n",
    "    empirical_distribution_function= #interpolate to get the eCDF\n",
    "   \n",
    "    pvalue= #initialize output array\n",
    "    for i in range(len(statistic)):\n",
    "        pvalue[i]= #probability of finding a larger value according to the null distribution\n",
    "        \n",
    "    return pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the p-values and visualize their distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "pvalue="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, one would consider as statistically significant a covariance value for which the p-value is smaller than a certain threshold fixed before the experiment.  This would be ok for a single neuron, but in our case we have to be more careful as we're testing multiple hypothesis simultaneously.\n",
    "\n",
    "What we can do is to control the false discovery rate, expected fraction of false positives, neurons that are incorrectly classified as significantly correlated.  We can implement the Benjamini–Hochberg procedure, which ensures that the false discovery rate is smaller than a given threshold $\\alpha$.  If we're testing $m$ hypotheses we have to sort the p-values in increasing order and find the largest index $k$ such that $P_k \\leq \\frac{k}{m}\\alpha$.  Then we  can reject the null hypothesis of uncorrelated signals for the $k$ neurons with the smallest p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can understand this procedure visually by plotting the p-values sorted with increasing order together with a line with slope $\\alpha/m$. Then we find the largest p-value that is below this line and we can reject the null hypothesis for all neurons whose p-values is smaller or equal to that one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to plot both the p-values and the Benjamini-Hochberg line as a function of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "EFJ5mhq-VfWo",
    "outputId": "1ce1a7dd-f08a-477e-e6e2-b07d95856949"
   },
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "alpha_bh=0.05\n",
    "n_neurons=len(pvalue)\n",
    "indices= #array containing integers from 1 to n_neurons\n",
    "sorted_pvalues= #p-values sorted in increasing order\n",
    "line_bh= #benjamini-hochberg line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to extract the indices of the significantly correlated neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_significant_neurons(statistic,statistic_bootstrap,alpha_bh=0.05):\n",
    "    \"\"\"Calculate p-values and test if they are significant using the Benjamini-Hochberg procedure.\n",
    "\n",
    "    Parameters :\n",
    "    ------------\n",
    "    statistic : 1d array\n",
    "        values of the statistic for which we obtained the p-values\n",
    "\n",
    "    statistic_bootstrap : 1d array\n",
    "        bootstrapped values of a statistic from which we estimated the null distribution\n",
    "        \n",
    "    alpha_bh : float\n",
    "        threshold on the false discovery rate\n",
    "        default : 0.05\n",
    "        \n",
    "    Return :\n",
    "    --------\n",
    "    indices_thr : 1d array\n",
    "        indices of neurons for which we reject the null hypothesis sorted with decreasing value of the statistic    \n",
    "\n",
    "    mask : 1d array\n",
    "        boolean array which gives True on the indices of neurons for which we reject the null hypothesis\n",
    "    \"\"\"\n",
    "    #⌨️⬇️\n",
    "    pvalue= #calculate p-values\n",
    "    \n",
    "    n_neurons=len(pvalue)\n",
    "    indices= #array containing integers from 1 to n_neurons\n",
    "    sorted_pvalues= #p-values sorted in increasing order\n",
    "    line_bh= #benjamini-hochberg line\n",
    "\n",
    "    indices_inequality= #find all indices for which the p-value is below the line\n",
    "    if len(indices_inequality)==0: #check if there are no p-values below the line\n",
    "        n_significant_neurons=0\n",
    "    else:\n",
    "        n_significant_neurons= #largest index\n",
    "\n",
    "    indices_sorted= #sort indices with decreasing value of the statistic\n",
    "    indices_thr= #only take the ones for which the statistic is significantly larger than zero\n",
    "    mask= #create a mask for the neurons for which we reject the null hypothesis\n",
    "    mask[ ]=1\n",
    "    mask=mask.astype(bool)\n",
    "\n",
    "    return indices_thr,mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see how many neurons are significantly correlated with the swimming strength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "indices_forward,mask_forward=\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting activity and position of significantly correlated neurons:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the fluorescence traces of the significantly correlated neurons together with the behavioral signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "plot_dff_traces()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you tell by looking at the activity traces?  Is there something wrong?  What do you think the problem is? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at all the fluorescence traces together by encoding them in the lightness values of the pixels in an image, the result is called a raster plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "plot_dff_raster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the positions of the neurons that are significantly correlated, with the opacity of the points proportional to the covariance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "plot_brain_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "plot_brain_projections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the activity of these neurons as a function of the swimming strength:\n",
    "\n",
    "To do so we have to discretize the range of values of the swimming strength (using `histogram_bin_edges` from numpy) and then calculate the average activity of these neurons when the swimming strength values fall in each of the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_dff(signal,dff,indices,n_bins=10):\n",
    "    \"\"\"Calculate average activity of certain neurons as a function of the signal values.\n",
    "\n",
    "    Parameters :\n",
    "    ------------\n",
    "    signal : 1d array\n",
    "        external signal, shape (n_timepoints)\n",
    "        \n",
    "    dff : 2d array\n",
    "        rescaled fluorescence traces for all neurons, shape (n_neurons,n_timepoints)\n",
    "        \n",
    "    indices : 1d array\n",
    "        indices of neurons over which we want to average\n",
    "        \n",
    "    Return :\n",
    "    --------\n",
    "    bin_centers : 1d array\n",
    "        values of signal at the center of each bin  \n",
    "\n",
    "    dff_mean : 1d array\n",
    "        average dff of indexed neurons for the signal in each of the bins\n",
    "        \n",
    "    dff_mean : 1d array\n",
    "        standard deviation of the dff values of indexed neurons for the signal in each of the bins\n",
    "    \"\"\"\n",
    "    #⌨️⬇️\n",
    "    bin_edges= #edges of the bins used to discretize the signal\n",
    "    bin_centers= #centers of th ebins\n",
    "    \n",
    "    dff_mean= #initialize average dff\n",
    "    dff_std= #and standard deviation\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        dff_mean[i]= #evaluate mean\n",
    "        dff_std[i]= #and standard deviation\n",
    "\n",
    "    return bin_centers,dff_mean,dff_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the average activity and plot it as a function of the swimming strength.  You can use the function `errorbar` from matplotlib to show the standard deviations together with the averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "bin_centers,dff_mean,dff_std="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuwYPT-7VfWz"
   },
   "source": [
    "### Correlation with stimulus speed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all this work we can easily look for neurons whose activity is correlated with the speed of the external stimulus.\n",
    "\n",
    "Let's start by preprocessing the signal and visualizing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "signal_speed="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "id": "mlq6XsDeVfW0",
    "outputId": "2e564a1f-19e7-439a-a66f-93701c1d6bd6"
   },
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's calculate the covariances and find the neurons for which they are significantly larger than zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KeYOhB7qVfW2",
    "outputId": "d21fd684-5231-475a-b7b4-71c5a655eb4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "covariance_speed=\n",
    "covariance_speed_bootstrap="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "indices_speed,mask_speed=\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally let's look at their activity and position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "plot_dff_traces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "plot_dff_raster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "plot_brain_projections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the results, are they as you expected? Is there something strange?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you have time you can try and analyze correlations with other signals that you think might be interesting, otherwise let's move on to the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen how to find neurons whose activity is correlated with a single signal, for example some feature of a behavior or of an external stimulus.\n",
    "\n",
    "But what about neurons that encode multiple different signal?\n",
    "\n",
    "We can use multiple linear regression to find how different external signals are encoded in the brain.\n",
    "\n",
    "We're going to fit neural activity independently for each neuron with a linear combination of such signals, called regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define regressors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that most neurons correlated with stimulus speed have a strong activation on the first trial.\n",
    "\n",
    "We can try to find how different neurons encode the surprise of the first unexpected trial, the presence of the stimulus and its speed, by defining three different regressor: one that is only active on the first trial, one active on all trials and one proportional to the speed of the stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an array containing all the regressors and visualize them. Be careful to normalize the three regressors so that we can better compare their relevance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "regressors="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot them to check that we defined them correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fit the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each neuron $n$ we assume that the fluorescence $y_n(t)$ can be expressed as a linear combination of the regressors $x_i(t)$ with certain coefficients $\\beta_{n,i}$ and some additional noise $\\epsilon(t)$:\n",
    "\n",
    "$$ y_n(t) = \\sum_{i} \\beta_{n,i} x_i(t) + \\epsilon(t)$$\n",
    "\n",
    "Then the coefficients of best fit $\\hat \\beta_{n,i}$ are those for which the squared error $\\sum_{t} (y_n(t)- \\sum_{i} \\beta_{n,i} x_i(t))^2$ is minimized.\n",
    "\n",
    "We can get them by using the `LinearRegression` class from the scikit-learn library.  \n",
    "\n",
    "Be careful about the shape of the arrays, you might have to consider the transposed arrays by using `.T`, which inverts the order of the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can look at the values of the coefficients for the three regressors across the brain by plotting the neurons with a colormap that encodes positive values in red and negative ones in blue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "plot_coefficients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the distribution of the coefficients using histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find neurons for which the fit is significantly good:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the coefficients of determination $R^2$ for each neuron, they're a meausure of how good the fit is.  \n",
    "\n",
    "$R^2$ is defined as the fraction of variance in the neural activity that is captured by our linear model:\n",
    "\n",
    "$$ R^2_n = 1 - \\frac{\\sum_{t}(y_n-\\sum_{i} \\hat \\beta_{n,i} x_i)^2}{\\sum_{t}(y_n-\\bar y_n)^2} $$\n",
    "\n",
    "You can use the function `r2_score` to compute the scores. Note that you can use the method `predict()` to obtain the fluorescence values $y_n$ predicted by the linear model for certain values of the independent variables $x_i$.  And be careful about the value of the `multioutput` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "r2="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we look at their distribution in the brain, we can choose a power of $R^2$ for the opacity in order to increase the contrast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "plot_brain_projections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the histogram of $R^2$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to find the neurons whose activity is well fitted by our linear model.  To find the $R^2$ values that are significantly different from zero we can again use the stationary bootstrap to create new samples of brain activity with a scrambled temporal structure and see how our model performs.\n",
    "\n",
    "Be careful about the shape of the input for the function `stationary_bootstrap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "n_resamples=50\n",
    "r2_sbs= #initialize array for bootstrapped r2 values\n",
    "\n",
    "for i in tqdm(range(n_resamples)):\n",
    "    dff_resampled= #resample dff traces with stationary bootstrap\n",
    "    reg= #perform linear regression on the resampled traces\n",
    "    r2_sbs[i*n_neurons:(i+1)*n_neurons]= #evaluate the r2 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the distribution of $R^2$ values with the null distribution obtained by bootstrapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Benjamini–Hochberg procedure once again to find the neurons for which $R^2$ is significantly larger than 0: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "indices_r2,mask_r2=\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look again at the value of the coefficients, but now just for the neurons for which the linear fit is significant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "plot_coefficients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analyze which regressor are important in fitting the activity of each neuron:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to find which combination of regressors is important for fitting the activity of each one of these neurons.\n",
    "\n",
    "We can estimate the uncertainties on the regression coefficient by using again a bootstrap method.\n",
    "This time we create new sets of (regressors,fluorescences) values by resampling them with replacement, here we don't care about preserving short term temporal structure in the data as we're keeping together values of regressors and fluorescences that correspond to the same time point. Therefore we don't have to use the stationary bootstrap, but a simple resampling.\n",
    "Then we can perform linear regression again on each of these resampled sets to obtain new estimates of the coefficients.\n",
    "Finally we can calculate the standard error of the coefficients as the sample standard deviation of the bootstrapped coefficient values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a function that takes a signal and resamples it with replacement to obtain the resampled_signalbootstrapped signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(signal):\n",
    "    \"\"\"resample a time series with replacement.\n",
    "\n",
    "    Parameters :\n",
    "    ------------\n",
    "    signal : array\n",
    "        original time series, the first dimension corresponds to time, shape (n_timepoints,...)\n",
    "        \n",
    "    Return :\n",
    "    --------\n",
    "    resampled_signal : array\n",
    "        bootstrapped time series\n",
    "    \"\"\"\n",
    "    #⌨️⬇️\n",
    "    l=len(signal)\n",
    "    random_indices= #l random numbers from 0 to l-1\n",
    "    resampled_signal= #resampled values from signal\n",
    "    return resampled_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to create new sets of (regressors,fluorescences) samples and get the best fit coefficients for each of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "n_resamples=100\n",
    "coef_bs= #initialize array for bootstrapped coefficients\n",
    "samples= #array containing regressors and dffs\n",
    "\n",
    "for n in tqdm(range(n_resamples)):\n",
    "    resampled= #bootstrapped samples\n",
    "    reg= #perform linear regression\n",
    "    coef_bs[n]= #get coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can estimate the standard error for each coefficient by evaluating the standard deviation of the bootstrapped samples, you can include a corrective factor of $\\sqrt{n/(n-1)}$ where $n$ is the number of bootstrapped samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "coef_se= #initialize the array\n",
    "for i in range(n_neurons):\n",
    "    coef_se[i]= #evaluate the standard error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the distributions of bootstrapped coefficients for a given neuron together with the actual value of the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=100\n",
    "\n",
    "colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "fig,ax=plt.subplots()\n",
    "for r in range(n_regressors):\n",
    "    ax.hist(coef_bs[:,i,r],bins='auto',histtype='step',density=True,color=colors[r])\n",
    "    ax.axvline(coef[i,r],color=colors[r])\n",
    "\n",
    "ax.set_xlabel('coefficients')\n",
    "ax.set_ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And have a look at the distributions of standard errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a label for each neuron in order to summarize the results of the multiple linear regression. For each regressor we assign $0$ if the corresponding coefficient is not significantly different from 0 (by comparing it with its standard error). For coefficients that are significantly different from 0 we assign $1$ if the coefficient is positive and $-1$ if it's negative.\n",
    "\n",
    "We say that a coefficient is significantly different from zero if it's further than zero by 2 times its standard error or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "neuron_labels= #initialize array\n",
    "for i in range(n_regressors): #loop over all regressors\n",
    "    neuron_labels[ ]=1 #if the coefficients are larger than 2*SE we set the sign to 1\n",
    "    neuron_labels[ ]=-1 #if they are smaller than -2*SE we set it to -1\n",
    "neuron_labels=neuron_labels[ ] #we only consider the neurons for which the fit is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can group the neurons using these labels and look at the frequency of each combination in the brain.\n",
    "\n",
    "We can get all possible combinations using `itertools.product`, and then count how many neurons are there for each combination: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "all_possible_labels= #get all possible combinations of 0, 1 and -1\n",
    "n_neurons_per_label=[]\n",
    "text_labels=[]\n",
    "for label in all_possible_labels:\n",
    "    n_neurons_per_label.append( ) #number of neurons with a certain label\n",
    "    text_labels.append( ) #each label saved as a string\n",
    "n_neurons_per_label=np.array(n_neurons_per_label)\n",
    "text_labels=np.array(text_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the number of neurons with each label with a bar graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n_neurons=0\n",
    "fig,ax=plt.subplots()\n",
    "ax.barh(text_labels[n_neurons_per_label>=min_n_neurons],n_neurons_per_label[n_neurons_per_label>=min_n_neurons])\n",
    "ax.set_xlabel('number of neurons')\n",
    "ax.set_ylabel('regression labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also order the labels with increasing number of neurons for clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "sorted_indices= #indices of the labels sorted by number of neurons per label\n",
    "text_labels_sorted= #text labels sorted by number of neurons\n",
    "n_neurons_per_label_sorted= #sorted number of neurons per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n_neurons=5\n",
    "fig,ax=plt.subplots()\n",
    "ax.barh(text_labels_sorted[n_neurons_per_label_sorted>=min_n_neurons],n_neurons_per_label_sorted[n_neurons_per_label_sorted>=min_n_neurons])\n",
    "ax.set_xlabel('number of neurons')\n",
    "ax.set_ylabel('regression labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each combination we can plot the position of the neurons with that label in the brain and their average activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "plot_neurons_per_label()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to look at the position of neurons with different labels, are they spatially organized?\n",
    "\n",
    "What about their average activities? Are they as you expected them?\n",
    "\n",
    "Are there some neuron with labels that you wouldn't have expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the activity of each individual neuron with a particular label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "label=(0,-1,1) #define a label\n",
    "indices_label= #find indices of all neurons with that label\n",
    "plot_dff_traces() #plot their activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other regressors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have some time you can try to perform multiple linear regression with other combinations of regressors.\n",
    "Otherwise let's move directly to the last section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we can define two regressors related to left and right tail movements.  The signal `side_thrust` is a measure of the strength of the tail movement towards the left and the right, positive values indicate thrust towards the left, while negative ones towards the right.\n",
    "\n",
    "We can generate two regressors by separating it into a signal with the swimming strength towards the left and one towards the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can simply rerun the cells in this section with a different choice for the `regressors` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Decoding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen how to model the activity of single neurons as a combination of external signals.\n",
    "\n",
    "Now we want to explore how to decode external signal from the activity of the whole brain.\n",
    "\n",
    "We'll try to apply again linear regression but this time we'll be using the activity of all neurons as regressors to try and estimate an external signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a target signal using the neural activities as regressors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to perform a regression using `stimulus_T` as the target signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the $R^2$ coefficient for the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize the target signal together with the prediction from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this result match your expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a perfect fit... And it's problematic because we're fitting a signal that doesn't have anything to do with our experiment.\n",
    "\n",
    "This phenomenon is called overfitting, we're able to fit any kind of signal because we have many more regressors (number of neurons ~ 30000) than data points (number of time points ~ 500). \n",
    "\n",
    "In order to test whether our model is good we should use only a part of the data points (training set) to obtain its parameters and then test how it performs on the remaining datapoints that we left out (test set).\n",
    "\n",
    "Let's try to fit the model on the first 80% of the signal and see how it performs on the remaining 20%.\n",
    "\n",
    "To create the training and test sets you can slice the original arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.arange(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "fraction_train= #fraction of timepoints in the training set\n",
    "idx= #index of the first point in the test set\n",
    "X_train= #neural activities in the training set\n",
    "X_test= #and test set\n",
    "y_train= #target signal values in the training set\n",
    "y_test= #and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the linear regression on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate $R^2$ on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize again the target signal together with the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected it doesn't work for predicting the second half of the experiment, it's reassuring!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to do the same but with a relevant target signal, the strength of the tail movements.  First define the new target values for the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "signal_forward= #normalize the signal\n",
    "\n",
    "y_train= #define traning set\n",
    "y_test= #and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then perform the regression and look at the $R^2$ scores on the two sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the predicted signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's much better but we're still overfitting the training set, in order to prevent this we'll have to use a technique called regularization.\n",
    "\n",
    "It consists in adding some constraints on the coefficients in order to reduce the freedom that allows the model to adapt too much to the training set.\n",
    "\n",
    "In practice we're going to introduce a term in our optimization problem that penalizes large values of the coefficients.\n",
    "Based on the specific functional form of this term we can have different results, two popular choices are the L2 (ridge) and the L1 (lasso) regulazion, corresponding to the square and the absolute value of the coefficients respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case the L1 is a good choice as it forces many coefficient to be exactly zero, and we expect many neurons to be irrelevant in encoding the target signal.\n",
    "\n",
    "Instead in our current model all neurons contribute to the linear combination, to get an idea we can visualize the distribution of coefficients in the brain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "plot_coefficients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a regularization means also adding an additional parameter $\\alpha$ which tunes the strength of the contraint.\n",
    "\n",
    "It's usually called a hyperparameter, meaning that it's not obtained from the data but we have to fix it ourselves.\n",
    "\n",
    "In order to choose a suitable value we can fit several model with different $\\alpha$s and then choose the one that performs better.\n",
    "\n",
    "It would be cheating to use the test set to decide the best value of $\\alpha$, as we're using the test set to decide the performance of our model.\n",
    "\n",
    "Therefore we're going to divide our training set further in two and create a validation set on which we can evaluate the model for different values of $\\alpha$s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_train= #fraction of timepoints in the training set\n",
    "idx= #index of the first point in the test set\n",
    "X_train= #neural activities in the training set\n",
    "X_test= #and test set\n",
    "y_train= #target signal values in the training set\n",
    "y_test= #and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "fraction_valid=0.2 #fraction of training set that we take for validation\n",
    "idx= #first index of the validation set\n",
    "\n",
    "X_train_alpha= #training set for hyperparameter tuning\n",
    "X_valid_alpha= #and validation set\n",
    "y_train_alpha= #same for target values\n",
    "y_valid_alpha="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can loop over some values of $\\alpha$ (we choose them logarithmically spaced to check over many different order of magnitude) and fit a linear model with L1 regularization for each value of $\\alpha$.  You can use the class `Lasso` from sklearn.  You might have to reduce the tolerance `tol` and increase the maximum number of iterations `max_iter` for the optimization to converge.  Let's save the $R^2$ scores on the validation set and the number of non-zero coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "alphas=np.logspace() #values of alpha that we want to test \n",
    "r2s_valid= #initialize array for validation scores\n",
    "n_nonzeros= #initialize array for number of non-zero coefficients\n",
    "\n",
    "for i in tqdm(range(len(alphas))):\n",
    "    model= #fit a linear model with L1 regularization\n",
    "    r2s_valid[i]= #score on the validation set\n",
    "    n_nonzeros[i]= #number of non-zero coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the $R^2$ the validation set in order to choose the best value of $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a look at the number of non-zero coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that they decrease as the strength of the regularization increases as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the best value for $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "best_alpha="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(r2s_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally train our model on the whole training set with our chosen value for $\\alpha$ and see how it performs on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And have a look at the predicted signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can look at the distribution of the coefficients in the brain, we can plot the histogram of the non-zero coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the values of the coefficients across the brain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#⌨️⬇️\n",
    "mask= #create a mask for the neurons which have non-zero coefficients\n",
    "mask[ ]=1\n",
    "mask=mask.astype(bool)\n",
    "\n",
    "plot_coefficients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that while the score on the training set decreased we have actually obtained a better score on the test set by using regularization.\n",
    "\n",
    "More importantly we see that most of the neurons are not necessary in decoding the target signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the meaning of neurons with negative coefficients?  You can try to plot their fluorescence traces.\n",
    "You can also try to perform a regression with the additional constraint of only positive coefficient by changing the parameter `positive`.  What changes in the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still have more time you try using L2 regularization and see what changes.  You can use the class `Ridge` from sklearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
